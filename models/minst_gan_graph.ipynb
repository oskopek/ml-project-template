{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: E:\\School\\ETH\\BoobsProject\\project\\models\n",
      "E:\\School\\ETH\\BoobsProject\\project\n"
     ]
    }
   ],
   "source": [
    "wd = %pwd\n",
    "print('Current directory:', wd)\n",
    "if wd.endswith('models'):\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os, time, datetime\n",
    "\n",
    "from models.base import BaseModel\n",
    "from resources.flags import FLAGS, define_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can only be run once.\n",
    "define_flags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the input data\n",
    "\n",
    "In this case, MNIST + batch and shuffle it. In our case, it will be quite different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device /gpu:0, and data format channels_first.\n",
      "Extracting data_in\\train-images-idx3-ubyte.gz\n",
      "Extracting data_in\\train-labels-idx1-ubyte.gz\n",
      "Extracting data_in\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data_in\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Reset Tensorflow graph and create new session\n",
    "tf.reset_default_graph()\n",
    "session = tf.Session()\n",
    "\n",
    "def shuffle(a, b):\n",
    "    # Generate the permutation index array.\n",
    "    permutation = np.random.permutation(a.shape[0])\n",
    "    # Shuffle the arrays by giving the permutation in the square brackets.\n",
    "    return a[permutation], b[permutation]\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Returns training and test tf.data.Dataset objects.\"\"\"\n",
    "    data = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "    #train_ds = tf.data.Dataset.from_tensor_slices((data.train.images,\n",
    "    #                                               data.train.labels))\n",
    "    #test_ds = tf.data.Dataset.from_tensors(\n",
    "    #   (data.test.images, data.test.labels))\n",
    "    return (data.train, data.test)\n",
    "\n",
    "device, data_format = ('/gpu:0', 'channels_first')\n",
    "#if FLAGS.no_gpu or tfe.num_gpus() <= 0:\n",
    "#    device, data_format = ('/cpu:0', 'channels_last')\n",
    "print('Using device %s, and data format %s.' % (device, data_format))\n",
    "\n",
    "# Load the datasets\n",
    "train_ds, test_ds = load_data(FLAGS.in_data_dir)\n",
    "#train_ds = train_ds.shuffle(60000).batch(FLAGS.batch_size)\n",
    "train_X, train_Y = shuffle(train_ds.images, train_ds.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup constatnts\n",
    "IMAGE_PIXELS=784\n",
    "NOISE_SIZE=100\n",
    "images_input = tf.placeholder(tf.float32, shape=(None, IMAGE_PIXELS))\n",
    "noise_input = tf.placeholder(tf.float32, shape=(None, NOISE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discriminator\n",
    "def discriminator(X, reuse):\n",
    "    with tf.variable_scope(\"Discriminator\", reuse=reuse):\n",
    "        # Layer 1\n",
    "        dx = tf.layers.dense(X, units=1024, kernel_initializer=tf.random_normal_initializer(stddev=0.02), activation=tf.nn.relu, name='fc1')\n",
    "        # Layer 2\n",
    "        #dx = tf.layers.dense(dx, units=512, activation=tf.nn.relu, name='fc2')\n",
    "        # Layer 3\n",
    "        #dx = tf.layers.dense(dx, units=256, activation=tf.nn.relu, name='fc3')\n",
    "        # Layer 4\n",
    "        d_out = tf.layers.dense(dx, units=1, kernel_initializer=tf.random_normal_initializer(stddev=0.02), name='fc_out')\n",
    "        return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generator\n",
    "def generator(X, reuse=False):\n",
    "    with tf.variable_scope('Generator', reuse=reuse):\n",
    "        # Layer 1\n",
    "        gx = tf.layers.dense(X, units=128, activation=tf.nn.relu, name='fc1')\n",
    "        # Layer 2\n",
    "        #gx = tf.layers.dense(gx, units=512, activation=tf.nn.relu, name='fc2')\n",
    "        # Layer 3\n",
    "        #gx = tf.layers.dense(gx, units=1024, activation=tf.nn.relu, name='fc3')\n",
    "        # Layer 4\n",
    "        g_out = tf.layers.dense(gx, units=784, activation=tf.nn.sigmoid, name='fc_out')\n",
    "        return g_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Losses\n",
    "g_sample = generator(noise_input)\n",
    "d_real = discriminator(images_input, reuse=False)\n",
    "d_fake = discriminator(g_sample, reuse=True)\n",
    "\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_real, labels=tf.ones_like(d_real)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_fake, labels=tf.zeros_like(d_fake)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_fake, labels=tf.ones_like(d_fake)))\n",
    "\n",
    "## Summaries\n",
    "gen_image_summary_op = tf.summary.image('generated_image0', tf.reshape(g_sample[0], (1, 28, 28, 1)), max_outputs=1)\n",
    "gen_image_summary_op1 = tf.summary.image('generated_image1', tf.reshape(g_sample[5], (1, 28, 28, 1)), max_outputs=1)\n",
    "gen_image_summary_op2 = tf.summary.image('generated_image2', tf.reshape(g_sample[10], (1, 28, 28, 1)), max_outputs=1)\n",
    "gen_image_summary_op3 = tf.summary.image('generated_image3', tf.reshape(g_sample[15], (1, 28, 28, 1)), max_outputs=1)\n",
    "\n",
    "# Optimizers\n",
    "t_vars = tf.trainable_variables()\n",
    "d_opt = tf.train.AdamOptimizer(2e-4).minimize(d_loss, var_list=[var for var in t_vars if 'Discriminator' in var.name])\n",
    "g_opt = tf.train.AdamOptimizer(2e-4).minimize(g_loss, var_list=[var for var in t_vars if 'Generator' in var.name])\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples=16\n",
    "test_noise = np.random.uniform(-1, 1, size=(num_test_samples, NOISE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "# Start interactive session\n",
    "session = tf.InteractiveSession()\n",
    "# Init Variables\n",
    "tf.global_variables_initializer().run()\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "# Create writer instance and write summary\n",
    "writer = tf.summary.FileWriter(logdir=FLAGS.out_data_dir + \"/gan_mnist_{}\".format(timestamp), graph=session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch: 0, Batch: 0, D_Loss: 1.4128124713897705, G_Loss: 1.0048991441726685\n",
      "Epoch: 0, Batch: 100, D_Loss: 0.01937485672533512, G_Loss: 5.944533824920654\n",
      "Epoch: 0, Batch: 200, D_Loss: 0.06171085312962532, G_Loss: 7.539041519165039\n",
      "Epoch: 0, Batch: 300, D_Loss: 0.11196392774581909, G_Loss: 5.143267631530762\n",
      "Epoch: 0, Batch: 400, D_Loss: 0.05813948065042496, G_Loss: 4.4670915603637695\n",
      "Epoch: 0, Batch: 500, D_Loss: 0.029366396367549896, G_Loss: 4.863171577453613\n",
      "Epoch 1\n",
      "Epoch: 1, Batch: 0, D_Loss: 0.08269477635622025, G_Loss: 4.6442060470581055\n",
      "Epoch: 1, Batch: 100, D_Loss: 0.01692943647503853, G_Loss: 5.350062370300293\n",
      "Epoch: 1, Batch: 200, D_Loss: 0.015572715550661087, G_Loss: 5.200153827667236\n",
      "Epoch: 1, Batch: 300, D_Loss: 0.02343159168958664, G_Loss: 4.266664028167725\n",
      "Epoch: 1, Batch: 400, D_Loss: 0.02806791476905346, G_Loss: 4.150373458862305\n",
      "Epoch: 1, Batch: 500, D_Loss: 0.04099668562412262, G_Loss: 4.437796592712402\n",
      "Epoch 2\n",
      "Epoch: 2, Batch: 0, D_Loss: 0.15111324191093445, G_Loss: 3.9040868282318115\n",
      "Epoch: 2, Batch: 100, D_Loss: 0.04431348294019699, G_Loss: 4.251264572143555\n",
      "Epoch: 2, Batch: 200, D_Loss: 0.04505855590105057, G_Loss: 4.133073806762695\n",
      "Epoch: 2, Batch: 300, D_Loss: 0.057083308696746826, G_Loss: 3.9194257259368896\n",
      "Epoch: 2, Batch: 400, D_Loss: 0.07008260488510132, G_Loss: 3.813934087753296\n",
      "Epoch: 2, Batch: 500, D_Loss: 0.07294810563325882, G_Loss: 4.476116180419922\n",
      "Epoch 3\n",
      "Epoch: 3, Batch: 0, D_Loss: 0.03921808674931526, G_Loss: 4.668583393096924\n",
      "Epoch: 3, Batch: 100, D_Loss: 0.03886996582150459, G_Loss: 4.8081889152526855\n",
      "Epoch: 3, Batch: 200, D_Loss: 0.022370029240846634, G_Loss: 5.298161029815674\n",
      "Epoch: 3, Batch: 300, D_Loss: 0.04730495810508728, G_Loss: 5.242913246154785\n",
      "Epoch: 3, Batch: 400, D_Loss: 0.03827666491270065, G_Loss: 5.282541275024414\n",
      "Epoch: 3, Batch: 500, D_Loss: 0.010030578821897507, G_Loss: 6.7229084968566895\n",
      "Epoch 4\n",
      "Epoch: 4, Batch: 0, D_Loss: 0.013119908049702644, G_Loss: 6.447337627410889\n",
      "Epoch: 4, Batch: 100, D_Loss: 0.012765919789671898, G_Loss: 6.981142520904541\n",
      "Epoch: 4, Batch: 200, D_Loss: 0.011913573369383812, G_Loss: 7.060919284820557\n",
      "Epoch: 4, Batch: 300, D_Loss: 0.009659838862717152, G_Loss: 6.851625442504883\n",
      "Epoch: 4, Batch: 400, D_Loss: 0.005085495300590992, G_Loss: 7.4499897956848145\n",
      "Epoch: 4, Batch: 500, D_Loss: 0.026456892490386963, G_Loss: 6.844752311706543\n",
      "Epoch 5\n",
      "Epoch: 5, Batch: 0, D_Loss: 0.037598222494125366, G_Loss: 7.086810111999512\n",
      "Epoch: 5, Batch: 100, D_Loss: 0.019136639311909676, G_Loss: 6.3005547523498535\n",
      "Epoch: 5, Batch: 200, D_Loss: 0.006635775323957205, G_Loss: 6.762819766998291\n",
      "Epoch: 5, Batch: 300, D_Loss: 0.00825236551463604, G_Loss: 6.980775356292725\n",
      "Epoch: 5, Batch: 400, D_Loss: 0.009212721139192581, G_Loss: 7.99815559387207\n",
      "Epoch: 5, Batch: 500, D_Loss: 0.004187430255115032, G_Loss: 7.18189811706543\n",
      "Epoch 6\n",
      "Epoch: 6, Batch: 0, D_Loss: 0.006906829308718443, G_Loss: 6.58914041519165\n",
      "Epoch: 6, Batch: 100, D_Loss: 0.006574470084160566, G_Loss: 6.731484413146973\n",
      "Epoch: 6, Batch: 200, D_Loss: 0.016621306538581848, G_Loss: 5.387015342712402\n",
      "Epoch: 6, Batch: 300, D_Loss: 0.005518767051398754, G_Loss: 6.5077409744262695\n",
      "Epoch: 6, Batch: 400, D_Loss: 0.017140217125415802, G_Loss: 6.361733436584473\n",
      "Epoch: 6, Batch: 500, D_Loss: 0.014337475411593914, G_Loss: 6.1791181564331055\n",
      "Epoch 7\n",
      "Epoch: 7, Batch: 0, D_Loss: 0.017788030207157135, G_Loss: 5.758464336395264\n",
      "Epoch: 7, Batch: 100, D_Loss: 0.03761990740895271, G_Loss: 4.313316822052002\n",
      "Epoch: 7, Batch: 200, D_Loss: 0.07158633321523666, G_Loss: 4.254021167755127\n",
      "Epoch: 7, Batch: 300, D_Loss: 0.019682280719280243, G_Loss: 5.675494194030762\n",
      "Epoch: 7, Batch: 400, D_Loss: 0.0916157066822052, G_Loss: 4.241207122802734\n",
      "Epoch: 7, Batch: 500, D_Loss: 0.02180377021431923, G_Loss: 5.527053356170654\n",
      "Epoch 8\n",
      "Epoch: 8, Batch: 0, D_Loss: 0.02240452915430069, G_Loss: 5.551548480987549\n",
      "Epoch: 8, Batch: 100, D_Loss: 0.022005941718816757, G_Loss: 5.229883193969727\n",
      "Epoch: 8, Batch: 200, D_Loss: 0.05142970755696297, G_Loss: 5.123912334442139\n",
      "Epoch: 8, Batch: 300, D_Loss: 0.016916317865252495, G_Loss: 5.829075813293457\n",
      "Epoch: 8, Batch: 400, D_Loss: 0.025134794414043427, G_Loss: 5.243051528930664\n",
      "Epoch: 8, Batch: 500, D_Loss: 0.026740681380033493, G_Loss: 5.387694835662842\n",
      "Epoch 9\n",
      "Epoch: 9, Batch: 0, D_Loss: 0.03253016620874405, G_Loss: 5.193708419799805\n",
      "Epoch: 9, Batch: 100, D_Loss: 0.03228617087006569, G_Loss: 5.143269062042236\n",
      "Epoch: 9, Batch: 200, D_Loss: 0.05062832683324814, G_Loss: 5.604482650756836\n",
      "Epoch: 9, Batch: 300, D_Loss: 0.022251814603805542, G_Loss: 6.312744617462158\n",
      "Epoch: 9, Batch: 400, D_Loss: 0.026398103684186935, G_Loss: 5.5498199462890625\n",
      "Epoch: 9, Batch: 500, D_Loss: 0.044806502759456635, G_Loss: 5.706142425537109\n",
      "Epoch 10\n",
      "Epoch: 10, Batch: 0, D_Loss: 0.019117632880806923, G_Loss: 6.122338771820068\n",
      "Epoch: 10, Batch: 100, D_Loss: 0.02600555308163166, G_Loss: 5.334170341491699\n",
      "Epoch: 10, Batch: 200, D_Loss: 0.013988595455884933, G_Loss: 6.009564399719238\n",
      "Epoch: 10, Batch: 300, D_Loss: 0.01807412877678871, G_Loss: 5.30277156829834\n",
      "Epoch: 10, Batch: 400, D_Loss: 0.04512117803096771, G_Loss: 5.343192100524902\n",
      "Epoch: 10, Batch: 500, D_Loss: 0.04322413355112076, G_Loss: 5.409501552581787\n",
      "Epoch 11\n",
      "Epoch: 11, Batch: 0, D_Loss: 0.02033940702676773, G_Loss: 5.853404998779297\n",
      "Epoch: 11, Batch: 100, D_Loss: 0.05201343446969986, G_Loss: 4.867446422576904\n",
      "Epoch: 11, Batch: 200, D_Loss: 0.04952869936823845, G_Loss: 4.657658576965332\n",
      "Epoch: 11, Batch: 300, D_Loss: 0.06389761716127396, G_Loss: 4.824375152587891\n",
      "Epoch: 11, Batch: 400, D_Loss: 0.0352509543299675, G_Loss: 5.55976676940918\n",
      "Epoch: 11, Batch: 500, D_Loss: 0.06559713184833527, G_Loss: 5.380399703979492\n",
      "Epoch 12\n",
      "Epoch: 12, Batch: 0, D_Loss: 0.01921187900006771, G_Loss: 6.037208080291748\n",
      "Epoch: 12, Batch: 100, D_Loss: 0.025343086570501328, G_Loss: 5.657773494720459\n",
      "Epoch: 12, Batch: 200, D_Loss: 0.011087030172348022, G_Loss: 6.034286022186279\n",
      "Epoch: 12, Batch: 300, D_Loss: 0.07085791230201721, G_Loss: 5.479901313781738\n",
      "Epoch: 12, Batch: 400, D_Loss: 0.024424493312835693, G_Loss: 6.235670566558838\n",
      "Epoch: 12, Batch: 500, D_Loss: 0.04564718157052994, G_Loss: 5.873823642730713\n",
      "Epoch 13\n",
      "Epoch: 13, Batch: 0, D_Loss: 0.019547665491700172, G_Loss: 6.786265850067139\n",
      "Epoch: 13, Batch: 100, D_Loss: 0.03525175526738167, G_Loss: 6.222824573516846\n",
      "Epoch: 13, Batch: 200, D_Loss: 0.015315128490328789, G_Loss: 6.540931224822998\n",
      "Epoch: 13, Batch: 300, D_Loss: 0.058775387704372406, G_Loss: 5.910905838012695\n",
      "Epoch: 13, Batch: 400, D_Loss: 0.016011863946914673, G_Loss: 6.363743782043457\n",
      "Epoch: 13, Batch: 500, D_Loss: 0.05027245730161667, G_Loss: 6.103205680847168\n",
      "Epoch 14\n",
      "Epoch: 14, Batch: 0, D_Loss: 0.034229107201099396, G_Loss: 7.256723403930664\n",
      "Epoch: 14, Batch: 100, D_Loss: 0.07096140831708908, G_Loss: 5.092973232269287\n",
      "Epoch: 14, Batch: 200, D_Loss: 0.02008979208767414, G_Loss: 6.224958419799805\n",
      "Epoch: 14, Batch: 300, D_Loss: 0.03936852142214775, G_Loss: 7.591996669769287\n",
      "Epoch: 14, Batch: 400, D_Loss: 0.03469925373792648, G_Loss: 5.520871639251709\n",
      "Epoch: 14, Batch: 500, D_Loss: 0.04550834000110626, G_Loss: 5.721633434295654\n",
      "Epoch 15\n",
      "Epoch: 15, Batch: 0, D_Loss: 0.01994876191020012, G_Loss: 6.535265922546387\n",
      "Epoch: 15, Batch: 100, D_Loss: 0.06874856352806091, G_Loss: 6.1039018630981445\n",
      "Epoch: 15, Batch: 200, D_Loss: 0.030155185610055923, G_Loss: 6.574901103973389\n",
      "Epoch: 15, Batch: 300, D_Loss: 0.021467706188559532, G_Loss: 5.928646087646484\n",
      "Epoch: 15, Batch: 400, D_Loss: 0.04523646831512451, G_Loss: 6.401313304901123\n",
      "Epoch: 15, Batch: 500, D_Loss: 0.04460994154214859, G_Loss: 7.093145847320557\n",
      "Epoch 16\n",
      "Epoch: 16, Batch: 0, D_Loss: 0.02465733140707016, G_Loss: 7.254004955291748\n",
      "Epoch: 16, Batch: 100, D_Loss: 0.0659838542342186, G_Loss: 6.662973403930664\n",
      "Epoch: 16, Batch: 200, D_Loss: 0.05161463841795921, G_Loss: 5.824865818023682\n",
      "Epoch: 16, Batch: 300, D_Loss: 0.06887078285217285, G_Loss: 8.186092376708984\n",
      "Epoch: 16, Batch: 400, D_Loss: 0.04414142668247223, G_Loss: 5.0271124839782715\n",
      "Epoch: 16, Batch: 500, D_Loss: 0.046293117105960846, G_Loss: 5.98222541809082\n",
      "Epoch 17\n",
      "Epoch: 17, Batch: 0, D_Loss: 0.019749198108911514, G_Loss: 6.462484359741211\n",
      "Epoch: 17, Batch: 100, D_Loss: 0.026621855795383453, G_Loss: 6.0945725440979\n",
      "Epoch: 17, Batch: 200, D_Loss: 0.05001204460859299, G_Loss: 5.439931869506836\n",
      "Epoch: 17, Batch: 300, D_Loss: 0.029508739709854126, G_Loss: 6.440112113952637\n",
      "Epoch: 17, Batch: 400, D_Loss: 0.050941843539476395, G_Loss: 5.907831192016602\n",
      "Epoch: 17, Batch: 500, D_Loss: 0.02068212628364563, G_Loss: 6.364700794219971\n",
      "Epoch 18\n",
      "Epoch: 18, Batch: 0, D_Loss: 0.08088746666908264, G_Loss: 6.070889472961426\n",
      "Epoch: 18, Batch: 100, D_Loss: 0.08288861811161041, G_Loss: 6.198509693145752\n",
      "Epoch: 18, Batch: 200, D_Loss: 0.0669785887002945, G_Loss: 5.376773834228516\n",
      "Epoch: 18, Batch: 300, D_Loss: 0.029736965894699097, G_Loss: 6.537578105926514\n",
      "Epoch: 18, Batch: 400, D_Loss: 0.08448352664709091, G_Loss: 6.127336025238037\n",
      "Epoch: 18, Batch: 500, D_Loss: 0.06444644927978516, G_Loss: 5.589841365814209\n",
      "Epoch 19\n",
      "Epoch: 19, Batch: 0, D_Loss: 0.04674726724624634, G_Loss: 6.147550106048584\n",
      "Epoch: 19, Batch: 100, D_Loss: 0.03364383429288864, G_Loss: 6.333737850189209\n",
      "Epoch: 19, Batch: 200, D_Loss: 0.03209546208381653, G_Loss: 5.737700939178467\n",
      "Epoch: 19, Batch: 300, D_Loss: 0.02697467803955078, G_Loss: 6.246599197387695\n",
      "Epoch: 19, Batch: 400, D_Loss: 0.05202764272689819, G_Loss: 6.302773475646973\n",
      "Epoch: 19, Batch: 500, D_Loss: 0.053574129939079285, G_Loss: 6.726768970489502\n",
      "Epoch 20\n",
      "Epoch: 20, Batch: 0, D_Loss: 0.05559558421373367, G_Loss: 6.011186599731445\n",
      "Epoch: 20, Batch: 100, D_Loss: 0.04393349587917328, G_Loss: 6.041069984436035\n",
      "Epoch: 20, Batch: 200, D_Loss: 0.07242795825004578, G_Loss: 5.688982009887695\n",
      "Epoch: 20, Batch: 300, D_Loss: 0.011773533187806606, G_Loss: 6.377432823181152\n",
      "Epoch: 20, Batch: 400, D_Loss: 0.028357017785310745, G_Loss: 5.637296199798584\n",
      "Epoch: 20, Batch: 500, D_Loss: 0.0992264524102211, G_Loss: 6.548391342163086\n",
      "Epoch 21\n",
      "Epoch: 21, Batch: 0, D_Loss: 0.056884974241256714, G_Loss: 5.658950328826904\n",
      "Epoch: 21, Batch: 100, D_Loss: 0.022551417350769043, G_Loss: 7.248012542724609\n",
      "Epoch: 21, Batch: 200, D_Loss: 0.042665932327508926, G_Loss: 5.960759162902832\n",
      "Epoch: 21, Batch: 300, D_Loss: 0.01550527848303318, G_Loss: 6.530916690826416\n",
      "Epoch: 21, Batch: 400, D_Loss: 0.05719775706529617, G_Loss: 5.830488681793213\n",
      "Epoch: 21, Batch: 500, D_Loss: 0.09529649466276169, G_Loss: 5.946520805358887\n",
      "Epoch 22\n",
      "Epoch: 22, Batch: 0, D_Loss: 0.06130539998412132, G_Loss: 6.53922176361084\n",
      "Epoch: 22, Batch: 100, D_Loss: 0.030409395694732666, G_Loss: 6.6060309410095215\n",
      "Epoch: 22, Batch: 200, D_Loss: 0.038645967841148376, G_Loss: 6.127408504486084\n",
      "Epoch: 22, Batch: 300, D_Loss: 0.05531112849712372, G_Loss: 6.32888126373291\n",
      "Epoch: 22, Batch: 400, D_Loss: 0.033595696091651917, G_Loss: 6.387880325317383\n",
      "Epoch: 22, Batch: 500, D_Loss: 0.0209797490388155, G_Loss: 8.073116302490234\n",
      "Epoch 23\n",
      "Epoch: 23, Batch: 0, D_Loss: 0.011084243655204773, G_Loss: 7.684877872467041\n",
      "Epoch: 23, Batch: 100, D_Loss: 0.02904370427131653, G_Loss: 6.600818634033203\n",
      "Epoch: 23, Batch: 200, D_Loss: 0.03757208585739136, G_Loss: 7.3717041015625\n",
      "Epoch: 23, Batch: 300, D_Loss: 0.02302565425634384, G_Loss: 7.055389881134033\n",
      "Epoch: 23, Batch: 400, D_Loss: 0.022140292450785637, G_Loss: 8.198497772216797\n",
      "Epoch: 23, Batch: 500, D_Loss: 0.04101671278476715, G_Loss: 8.198342323303223\n",
      "Epoch 24\n",
      "Epoch: 24, Batch: 0, D_Loss: 0.012934675440192223, G_Loss: 7.756905555725098\n",
      "Epoch: 24, Batch: 100, D_Loss: 0.053723111748695374, G_Loss: 7.040421009063721\n",
      "Epoch: 24, Batch: 200, D_Loss: 0.0561232790350914, G_Loss: 5.787559986114502\n",
      "Epoch: 24, Batch: 300, D_Loss: 0.04365798085927963, G_Loss: 5.626285552978516\n",
      "Epoch: 24, Batch: 400, D_Loss: 0.04339625686407089, G_Loss: 6.760184288024902\n",
      "Epoch: 24, Batch: 500, D_Loss: 0.02021951414644718, G_Loss: 6.992724418640137\n",
      "Epoch 25\n",
      "Epoch: 25, Batch: 0, D_Loss: 0.03096623718738556, G_Loss: 7.046831130981445\n",
      "Epoch: 25, Batch: 100, D_Loss: 0.03601891174912453, G_Loss: 6.474895000457764\n",
      "Epoch: 25, Batch: 200, D_Loss: 0.026791861280798912, G_Loss: 6.658285140991211\n",
      "Epoch: 25, Batch: 300, D_Loss: 0.013916030526161194, G_Loss: 7.000192642211914\n",
      "Epoch: 25, Batch: 400, D_Loss: 0.04632168635725975, G_Loss: 6.753832817077637\n",
      "Epoch: 25, Batch: 500, D_Loss: 0.0601639449596405, G_Loss: 6.636104583740234\n",
      "Epoch 26\n",
      "Epoch: 26, Batch: 0, D_Loss: 0.03861566632986069, G_Loss: 6.994764804840088\n",
      "Epoch: 26, Batch: 100, D_Loss: 0.05083885416388512, G_Loss: 6.2945556640625\n",
      "Epoch: 26, Batch: 200, D_Loss: 0.030092313885688782, G_Loss: 5.952932357788086\n",
      "Epoch: 26, Batch: 300, D_Loss: 0.06758128851652145, G_Loss: 5.443886756896973\n",
      "Epoch: 26, Batch: 400, D_Loss: 0.053435713052749634, G_Loss: 5.655975818634033\n",
      "Epoch: 26, Batch: 500, D_Loss: 0.04641643539071083, G_Loss: 6.384622573852539\n",
      "Epoch 27\n",
      "Epoch: 27, Batch: 0, D_Loss: 0.03432651609182358, G_Loss: 6.130344390869141\n",
      "Epoch: 27, Batch: 100, D_Loss: 0.06884193420410156, G_Loss: 4.717846870422363\n",
      "Epoch: 27, Batch: 200, D_Loss: 0.021864935755729675, G_Loss: 6.498190879821777\n",
      "Epoch: 27, Batch: 300, D_Loss: 0.03305722773075104, G_Loss: 6.333528995513916\n",
      "Epoch: 27, Batch: 400, D_Loss: 0.04124089702963829, G_Loss: 5.553049087524414\n",
      "Epoch: 27, Batch: 500, D_Loss: 0.02357686683535576, G_Loss: 6.526673793792725\n",
      "Epoch 28\n",
      "Epoch: 28, Batch: 0, D_Loss: 0.041911061853170395, G_Loss: 6.559166431427002\n",
      "Epoch: 28, Batch: 100, D_Loss: 0.0474797859787941, G_Loss: 5.764868259429932\n",
      "Epoch: 28, Batch: 200, D_Loss: 0.05945975333452225, G_Loss: 5.135851860046387\n",
      "Epoch: 28, Batch: 300, D_Loss: 0.04775224253535271, G_Loss: 5.2856645584106445\n",
      "Epoch: 28, Batch: 400, D_Loss: 0.06269478797912598, G_Loss: 5.39718770980835\n",
      "Epoch: 28, Batch: 500, D_Loss: 0.0839986577630043, G_Loss: 5.416963577270508\n",
      "Epoch 29\n",
      "Epoch: 29, Batch: 0, D_Loss: 0.055437542498111725, G_Loss: 6.43382453918457\n",
      "Epoch: 29, Batch: 100, D_Loss: 0.03133662790060043, G_Loss: 5.594433784484863\n",
      "Epoch: 29, Batch: 200, D_Loss: 0.06077372282743454, G_Loss: 5.370542049407959\n",
      "Epoch: 29, Batch: 300, D_Loss: 0.05090808868408203, G_Loss: 5.567010402679443\n",
      "Epoch: 29, Batch: 400, D_Loss: 0.1109565868973732, G_Loss: 5.683816909790039\n",
      "Epoch: 29, Batch: 500, D_Loss: 0.03976711630821228, G_Loss: 6.282015323638916\n",
      "Epoch 30\n",
      "Epoch: 30, Batch: 0, D_Loss: 0.016714444383978844, G_Loss: 6.9849066734313965\n",
      "Epoch: 30, Batch: 100, D_Loss: 0.013388736173510551, G_Loss: 6.608877658843994\n",
      "Epoch: 30, Batch: 200, D_Loss: 0.08498240262269974, G_Loss: 6.758793354034424\n",
      "Epoch: 30, Batch: 300, D_Loss: 0.057930756360292435, G_Loss: 5.198145866394043\n",
      "Epoch: 30, Batch: 400, D_Loss: 0.05235864967107773, G_Loss: 4.621411323547363\n",
      "Epoch: 30, Batch: 500, D_Loss: 0.05112570896744728, G_Loss: 5.245315074920654\n",
      "Epoch 31\n",
      "Epoch: 31, Batch: 0, D_Loss: 0.03299275040626526, G_Loss: 5.725299835205078\n",
      "Epoch: 31, Batch: 100, D_Loss: 0.07819570600986481, G_Loss: 5.391363620758057\n",
      "Epoch: 31, Batch: 200, D_Loss: 0.11129733920097351, G_Loss: 5.764922618865967\n",
      "Epoch: 31, Batch: 300, D_Loss: 0.028941601514816284, G_Loss: 6.6632537841796875\n",
      "Epoch: 31, Batch: 400, D_Loss: 0.035164982080459595, G_Loss: 6.400385856628418\n",
      "Epoch: 31, Batch: 500, D_Loss: 0.030638612806797028, G_Loss: 6.169053554534912\n",
      "Epoch 32\n",
      "Epoch: 32, Batch: 0, D_Loss: 0.010654745623469353, G_Loss: 7.882331371307373\n",
      "Epoch: 32, Batch: 100, D_Loss: 0.04262837767601013, G_Loss: 6.654458045959473\n",
      "Epoch: 32, Batch: 200, D_Loss: 0.057529836893081665, G_Loss: 6.253727912902832\n",
      "Epoch: 32, Batch: 300, D_Loss: 0.09741649031639099, G_Loss: 5.104984283447266\n",
      "Epoch: 32, Batch: 400, D_Loss: 0.0621977299451828, G_Loss: 4.829000949859619\n",
      "Epoch: 32, Batch: 500, D_Loss: 0.02554202824831009, G_Loss: 6.707841873168945\n",
      "Epoch 33\n",
      "Epoch: 33, Batch: 0, D_Loss: 0.04166167974472046, G_Loss: 7.078141689300537\n",
      "Epoch: 33, Batch: 100, D_Loss: 0.07403160631656647, G_Loss: 5.0777506828308105\n",
      "Epoch: 33, Batch: 200, D_Loss: 0.03575954586267471, G_Loss: 6.884060859680176\n",
      "Epoch: 33, Batch: 300, D_Loss: 0.012261824682354927, G_Loss: 6.972834587097168\n",
      "Epoch: 33, Batch: 400, D_Loss: 0.05502752959728241, G_Loss: 5.8514838218688965\n",
      "Epoch: 33, Batch: 500, D_Loss: 0.05372769758105278, G_Loss: 5.398718357086182\n",
      "Epoch 34\n",
      "Epoch: 34, Batch: 0, D_Loss: 0.0517718531191349, G_Loss: 7.022174835205078\n",
      "Epoch: 34, Batch: 100, D_Loss: 0.12696479260921478, G_Loss: 4.837276458740234\n",
      "Epoch: 34, Batch: 200, D_Loss: 0.03175878897309303, G_Loss: 5.89478874206543\n",
      "Epoch: 34, Batch: 300, D_Loss: 0.07174341380596161, G_Loss: 7.075861930847168\n",
      "Epoch: 34, Batch: 400, D_Loss: 0.07914665341377258, G_Loss: 4.677362442016602\n",
      "Epoch: 34, Batch: 500, D_Loss: 0.09041288495063782, G_Loss: 6.446505069732666\n",
      "Epoch 35\n",
      "Epoch: 35, Batch: 0, D_Loss: 0.07806946337223053, G_Loss: 6.507506847381592\n",
      "Epoch: 35, Batch: 100, D_Loss: 0.03886735439300537, G_Loss: 4.992641448974609\n",
      "Epoch: 35, Batch: 200, D_Loss: 0.09373261034488678, G_Loss: 4.866523265838623\n",
      "Epoch: 35, Batch: 300, D_Loss: 0.049266815185546875, G_Loss: 6.533735275268555\n",
      "Epoch: 35, Batch: 400, D_Loss: 0.06183454394340515, G_Loss: 5.966541767120361\n",
      "Epoch: 35, Batch: 500, D_Loss: 0.11945244669914246, G_Loss: 6.743464946746826\n",
      "Epoch 36\n",
      "Epoch: 36, Batch: 0, D_Loss: 0.05142500251531601, G_Loss: 6.127216815948486\n",
      "Epoch: 36, Batch: 100, D_Loss: 0.052969347685575485, G_Loss: 4.65744161605835\n",
      "Epoch: 36, Batch: 200, D_Loss: 0.05853215605020523, G_Loss: 5.090878009796143\n",
      "Epoch: 36, Batch: 300, D_Loss: 0.07381600141525269, G_Loss: 7.176007270812988\n",
      "Epoch: 36, Batch: 400, D_Loss: 0.0906670093536377, G_Loss: 4.648807048797607\n",
      "Epoch: 36, Batch: 500, D_Loss: 0.060215163975954056, G_Loss: 5.823157787322998\n",
      "Epoch 37\n",
      "Epoch: 37, Batch: 0, D_Loss: 0.12727275490760803, G_Loss: 5.893226146697998\n",
      "Epoch: 37, Batch: 100, D_Loss: 0.02780238911509514, G_Loss: 5.589165687561035\n",
      "Epoch: 37, Batch: 200, D_Loss: 0.0899428203701973, G_Loss: 5.413445949554443\n",
      "Epoch: 37, Batch: 300, D_Loss: 0.08538557589054108, G_Loss: 5.752606391906738\n",
      "Epoch: 37, Batch: 400, D_Loss: 0.07523776590824127, G_Loss: 5.170327186584473\n",
      "Epoch: 37, Batch: 500, D_Loss: 0.07971156388521194, G_Loss: 6.065629959106445\n",
      "Epoch 38\n",
      "Epoch: 38, Batch: 0, D_Loss: 0.10852152854204178, G_Loss: 5.98753547668457\n",
      "Epoch: 38, Batch: 100, D_Loss: 0.10694144666194916, G_Loss: 5.189846992492676\n",
      "Epoch: 38, Batch: 200, D_Loss: 0.09961502254009247, G_Loss: 6.092856407165527\n",
      "Epoch: 38, Batch: 300, D_Loss: 0.051023904234170914, G_Loss: 5.120322227478027\n",
      "Epoch: 38, Batch: 400, D_Loss: 0.08746333420276642, G_Loss: 5.490530014038086\n",
      "Epoch: 38, Batch: 500, D_Loss: 0.11319397389888763, G_Loss: 5.692375659942627\n",
      "Epoch 39\n",
      "Epoch: 39, Batch: 0, D_Loss: 0.08736389130353928, G_Loss: 6.2852864265441895\n",
      "Epoch: 39, Batch: 100, D_Loss: 0.06705450266599655, G_Loss: 5.884660720825195\n",
      "Epoch: 39, Batch: 200, D_Loss: 0.0922136977314949, G_Loss: 5.886638164520264\n",
      "Epoch: 39, Batch: 300, D_Loss: 0.06670086085796356, G_Loss: 5.348086357116699\n",
      "Epoch: 39, Batch: 400, D_Loss: 0.04801823943853378, G_Loss: 5.643794059753418\n",
      "Epoch: 39, Batch: 500, D_Loss: 0.060385651886463165, G_Loss: 5.823307514190674\n",
      "Epoch 40\n",
      "Epoch: 40, Batch: 0, D_Loss: 0.02267693169414997, G_Loss: 6.916876316070557\n",
      "Epoch: 40, Batch: 100, D_Loss: 0.07466699928045273, G_Loss: 5.809454917907715\n",
      "Epoch: 40, Batch: 200, D_Loss: 0.09783458709716797, G_Loss: 6.856348991394043\n",
      "Epoch: 40, Batch: 300, D_Loss: 0.04442049562931061, G_Loss: 6.123661994934082\n",
      "Epoch: 40, Batch: 400, D_Loss: 0.11589717864990234, G_Loss: 5.29327392578125\n",
      "Epoch: 40, Batch: 500, D_Loss: 0.14256398379802704, G_Loss: 6.586244106292725\n",
      "Epoch 41\n",
      "Epoch: 41, Batch: 0, D_Loss: 0.1875443458557129, G_Loss: 5.372258186340332\n",
      "Epoch: 41, Batch: 100, D_Loss: 0.11466635763645172, G_Loss: 5.319157600402832\n",
      "Epoch: 41, Batch: 200, D_Loss: 0.07515859603881836, G_Loss: 6.542489051818848\n",
      "Epoch: 41, Batch: 300, D_Loss: 0.10586369037628174, G_Loss: 5.635530948638916\n",
      "Epoch: 41, Batch: 400, D_Loss: 0.09243816137313843, G_Loss: 5.834417819976807\n",
      "Epoch: 41, Batch: 500, D_Loss: 0.1426859349012375, G_Loss: 4.7139573097229\n",
      "Epoch 42\n",
      "Epoch: 42, Batch: 0, D_Loss: 0.10344802588224411, G_Loss: 5.236845016479492\n",
      "Epoch: 42, Batch: 100, D_Loss: 0.06709402799606323, G_Loss: 5.689565658569336\n",
      "Epoch: 42, Batch: 200, D_Loss: 0.17341500520706177, G_Loss: 5.919295787811279\n",
      "Epoch: 42, Batch: 300, D_Loss: 0.06317344307899475, G_Loss: 6.452965259552002\n",
      "Epoch: 42, Batch: 400, D_Loss: 0.05514759570360184, G_Loss: 4.927293300628662\n",
      "Epoch: 42, Batch: 500, D_Loss: 0.11775423586368561, G_Loss: 5.964194297790527\n",
      "Epoch 43\n",
      "Epoch: 43, Batch: 0, D_Loss: 0.1158657819032669, G_Loss: 5.304944038391113\n",
      "Epoch: 43, Batch: 100, D_Loss: 0.08246445655822754, G_Loss: 5.223611831665039\n",
      "Epoch: 43, Batch: 200, D_Loss: 0.07997703552246094, G_Loss: 5.242455005645752\n",
      "Epoch: 43, Batch: 300, D_Loss: 0.036396294832229614, G_Loss: 5.936623573303223\n",
      "Epoch: 43, Batch: 400, D_Loss: 0.08355221897363663, G_Loss: 5.627162933349609\n",
      "Epoch: 43, Batch: 500, D_Loss: 0.05324212461709976, G_Loss: 5.942010879516602\n",
      "Epoch 44\n",
      "Epoch: 44, Batch: 0, D_Loss: 0.10605267435312271, G_Loss: 5.359335899353027\n",
      "Epoch: 44, Batch: 100, D_Loss: 0.06881393492221832, G_Loss: 5.363120079040527\n",
      "Epoch: 44, Batch: 200, D_Loss: 0.07499517500400543, G_Loss: 4.817693710327148\n",
      "Epoch: 44, Batch: 300, D_Loss: 0.05484423413872719, G_Loss: 6.045767307281494\n",
      "Epoch: 44, Batch: 400, D_Loss: 0.08250617980957031, G_Loss: 6.032930850982666\n",
      "Epoch: 44, Batch: 500, D_Loss: 0.0850989818572998, G_Loss: 6.757566452026367\n",
      "Epoch 45\n",
      "Epoch: 45, Batch: 0, D_Loss: 0.12401548027992249, G_Loss: 5.527324199676514\n",
      "Epoch: 45, Batch: 100, D_Loss: 0.05911367014050484, G_Loss: 5.789018630981445\n",
      "Epoch: 45, Batch: 200, D_Loss: 0.08137719333171844, G_Loss: 5.384072303771973\n",
      "Epoch: 45, Batch: 300, D_Loss: 0.03630142658948898, G_Loss: 5.681063652038574\n",
      "Epoch: 45, Batch: 400, D_Loss: 0.09999442100524902, G_Loss: 5.263662338256836\n",
      "Epoch: 45, Batch: 500, D_Loss: 0.08311351388692856, G_Loss: 5.701991081237793\n",
      "Epoch 46\n",
      "Epoch: 46, Batch: 0, D_Loss: 0.207659512758255, G_Loss: 5.197485446929932\n",
      "Epoch: 46, Batch: 100, D_Loss: 0.062098387628793716, G_Loss: 4.924506664276123\n",
      "Epoch: 46, Batch: 200, D_Loss: 0.11377982795238495, G_Loss: 5.307386875152588\n",
      "Epoch: 46, Batch: 300, D_Loss: 0.05200699716806412, G_Loss: 5.534650802612305\n",
      "Epoch: 46, Batch: 400, D_Loss: 0.09949386119842529, G_Loss: 5.389160633087158\n",
      "Epoch: 46, Batch: 500, D_Loss: 0.08710173517465591, G_Loss: 6.403140068054199\n",
      "Epoch 47\n",
      "Epoch: 47, Batch: 0, D_Loss: 0.16890473663806915, G_Loss: 5.528921127319336\n",
      "Epoch: 47, Batch: 100, D_Loss: 0.08675190806388855, G_Loss: 4.872869491577148\n",
      "Epoch: 47, Batch: 200, D_Loss: 0.12213976681232452, G_Loss: 5.342128753662109\n",
      "Epoch: 47, Batch: 300, D_Loss: 0.04300682619214058, G_Loss: 5.462438583374023\n",
      "Epoch: 47, Batch: 400, D_Loss: 0.1128978580236435, G_Loss: 4.430248260498047\n",
      "Epoch: 47, Batch: 500, D_Loss: 0.12912964820861816, G_Loss: 6.152224540710449\n",
      "Epoch 48\n",
      "Epoch: 48, Batch: 0, D_Loss: 0.11374964565038681, G_Loss: 4.902805328369141\n",
      "Epoch: 48, Batch: 100, D_Loss: 0.09668941050767899, G_Loss: 5.604970932006836\n",
      "Epoch: 48, Batch: 200, D_Loss: 0.05635715276002884, G_Loss: 6.2689409255981445\n",
      "Epoch: 48, Batch: 300, D_Loss: 0.08196631819009781, G_Loss: 6.573851108551025\n",
      "Epoch: 48, Batch: 400, D_Loss: 0.08147592842578888, G_Loss: 5.231724262237549\n",
      "Epoch: 48, Batch: 500, D_Loss: 0.13867367804050446, G_Loss: 5.5982770919799805\n",
      "Epoch 49\n",
      "Epoch: 49, Batch: 0, D_Loss: 0.13222603499889374, G_Loss: 5.0810933113098145\n",
      "Epoch: 49, Batch: 100, D_Loss: 0.09548613429069519, G_Loss: 4.945230484008789\n",
      "Epoch: 49, Batch: 200, D_Loss: 0.058044299483299255, G_Loss: 5.528875827789307\n",
      "Epoch: 49, Batch: 300, D_Loss: 0.10447358340024948, G_Loss: 5.023738861083984\n",
      "Epoch: 49, Batch: 400, D_Loss: 0.09863395988941193, G_Loss: 5.161360263824463\n",
      "Epoch: 49, Batch: 500, D_Loss: 0.11577332019805908, G_Loss: 5.145538806915283\n",
      "Epoch 50\n",
      "Epoch: 50, Batch: 0, D_Loss: 0.1144203245639801, G_Loss: 4.851464748382568\n",
      "Epoch: 50, Batch: 100, D_Loss: 0.08053166419267654, G_Loss: 4.483463287353516\n",
      "Epoch: 50, Batch: 200, D_Loss: 0.05329475924372673, G_Loss: 4.984097480773926\n",
      "Epoch: 50, Batch: 300, D_Loss: 0.08937550336122513, G_Loss: 5.873416900634766\n",
      "Epoch: 50, Batch: 400, D_Loss: 0.11411430686712265, G_Loss: 5.553872108459473\n",
      "Epoch: 50, Batch: 500, D_Loss: 0.07326145470142365, G_Loss: 5.19352912902832\n",
      "Epoch 51\n",
      "Epoch: 51, Batch: 0, D_Loss: 0.17516495287418365, G_Loss: 5.125120639801025\n",
      "Epoch: 51, Batch: 100, D_Loss: 0.046324677765369415, G_Loss: 5.327786922454834\n",
      "Epoch: 51, Batch: 200, D_Loss: 0.07337747514247894, G_Loss: 5.382308483123779\n",
      "Epoch: 51, Batch: 300, D_Loss: 0.095553457736969, G_Loss: 5.527264595031738\n",
      "Epoch: 51, Batch: 400, D_Loss: 0.10493813455104828, G_Loss: 5.700766563415527\n",
      "Epoch: 51, Batch: 500, D_Loss: 0.0860302597284317, G_Loss: 6.282734394073486\n",
      "Epoch 52\n",
      "Epoch: 52, Batch: 0, D_Loss: 0.14307823777198792, G_Loss: 5.165609359741211\n",
      "Epoch: 52, Batch: 100, D_Loss: 0.07467612624168396, G_Loss: 5.393321514129639\n",
      "Epoch: 52, Batch: 200, D_Loss: 0.0960802510380745, G_Loss: 5.1214919090271\n",
      "Epoch: 52, Batch: 300, D_Loss: 0.0898076519370079, G_Loss: 5.007462978363037\n",
      "Epoch: 52, Batch: 400, D_Loss: 0.07847616076469421, G_Loss: 5.5173468589782715\n",
      "Epoch: 52, Batch: 500, D_Loss: 0.05284503847360611, G_Loss: 5.631644248962402\n",
      "Epoch 53\n",
      "Epoch: 53, Batch: 0, D_Loss: 0.1659952849149704, G_Loss: 4.147408485412598\n",
      "Epoch: 53, Batch: 100, D_Loss: 0.037641920149326324, G_Loss: 5.562410831451416\n",
      "Epoch: 53, Batch: 200, D_Loss: 0.056825753301382065, G_Loss: 5.471231460571289\n",
      "Epoch: 53, Batch: 300, D_Loss: 0.11900997161865234, G_Loss: 5.386649131774902\n",
      "Epoch: 53, Batch: 400, D_Loss: 0.10204412788152695, G_Loss: 4.6043009757995605\n",
      "Epoch: 53, Batch: 500, D_Loss: 0.08199870586395264, G_Loss: 5.339882850646973\n",
      "Epoch 54\n",
      "Epoch: 54, Batch: 0, D_Loss: 0.13523392379283905, G_Loss: 4.779910564422607\n",
      "Epoch: 54, Batch: 100, D_Loss: 0.0806015133857727, G_Loss: 5.357349872589111\n",
      "Epoch: 54, Batch: 200, D_Loss: 0.07081178575754166, G_Loss: 5.404155254364014\n",
      "Epoch: 54, Batch: 300, D_Loss: 0.05051002278923988, G_Loss: 5.0466389656066895\n",
      "Epoch: 54, Batch: 400, D_Loss: 0.0886901244521141, G_Loss: 5.857875823974609\n",
      "Epoch: 54, Batch: 500, D_Loss: 0.04882155731320381, G_Loss: 6.170243740081787\n",
      "Epoch 55\n",
      "Epoch: 55, Batch: 0, D_Loss: 0.12383000552654266, G_Loss: 5.182499408721924\n",
      "Epoch: 55, Batch: 100, D_Loss: 0.06392031908035278, G_Loss: 4.9013471603393555\n",
      "Epoch: 55, Batch: 200, D_Loss: 0.05398701876401901, G_Loss: 5.485362529754639\n",
      "Epoch: 55, Batch: 300, D_Loss: 0.04683256894350052, G_Loss: 6.297150135040283\n",
      "Epoch: 55, Batch: 400, D_Loss: 0.1431218385696411, G_Loss: 5.289231777191162\n",
      "Epoch: 55, Batch: 500, D_Loss: 0.03520277887582779, G_Loss: 6.669239044189453\n",
      "Epoch 56\n",
      "Epoch: 56, Batch: 0, D_Loss: 0.05912265181541443, G_Loss: 5.9009881019592285\n",
      "Epoch: 56, Batch: 100, D_Loss: 0.10863243043422699, G_Loss: 5.3415207862854\n",
      "Epoch: 56, Batch: 200, D_Loss: 0.07770410180091858, G_Loss: 5.487646579742432\n",
      "Epoch: 56, Batch: 300, D_Loss: 0.02529200352728367, G_Loss: 6.607443809509277\n",
      "Epoch: 56, Batch: 400, D_Loss: 0.018675217404961586, G_Loss: 7.768931865692139\n",
      "Epoch: 56, Batch: 500, D_Loss: 0.06672288477420807, G_Loss: 7.924079418182373\n",
      "Epoch 57\n",
      "Epoch: 57, Batch: 0, D_Loss: 0.10625559091567993, G_Loss: 6.46805477142334\n",
      "Epoch: 57, Batch: 100, D_Loss: 0.044147390872240067, G_Loss: 5.199532985687256\n",
      "Epoch: 57, Batch: 200, D_Loss: 0.11620128154754639, G_Loss: 4.839114189147949\n",
      "Epoch: 57, Batch: 300, D_Loss: 0.12526684999465942, G_Loss: 5.944989204406738\n",
      "Epoch: 57, Batch: 400, D_Loss: 0.10859186947345734, G_Loss: 6.711451530456543\n",
      "Epoch: 57, Batch: 500, D_Loss: 0.049929916858673096, G_Loss: 7.479313850402832\n",
      "Epoch 58\n",
      "Epoch: 58, Batch: 0, D_Loss: 0.06475568562746048, G_Loss: 5.876729965209961\n",
      "Epoch: 58, Batch: 100, D_Loss: 0.027067335322499275, G_Loss: 5.549719333648682\n",
      "Epoch: 58, Batch: 200, D_Loss: 0.11767461895942688, G_Loss: 4.773204803466797\n",
      "Epoch: 58, Batch: 300, D_Loss: 0.05542721226811409, G_Loss: 7.230636119842529\n",
      "Epoch: 58, Batch: 400, D_Loss: 0.07792140543460846, G_Loss: 5.270444393157959\n",
      "Epoch: 58, Batch: 500, D_Loss: 0.06913983821868896, G_Loss: 6.754989147186279\n",
      "Epoch 59\n",
      "Epoch: 59, Batch: 0, D_Loss: 0.10738356411457062, G_Loss: 6.156603813171387\n",
      "Epoch: 59, Batch: 100, D_Loss: 0.046787090599536896, G_Loss: 5.355340957641602\n",
      "Epoch: 59, Batch: 200, D_Loss: 0.17558646202087402, G_Loss: 5.113177299499512\n",
      "Epoch: 59, Batch: 300, D_Loss: 0.08111987262964249, G_Loss: 5.501619815826416\n",
      "Epoch: 59, Batch: 400, D_Loss: 0.13313457369804382, G_Loss: 5.371249198913574\n",
      "Epoch: 59, Batch: 500, D_Loss: 0.1057262048125267, G_Loss: 7.418064117431641\n",
      "Epoch 60\n",
      "Epoch: 60, Batch: 0, D_Loss: 0.10695385932922363, G_Loss: 6.633951187133789\n",
      "Epoch: 60, Batch: 100, D_Loss: 0.0342467799782753, G_Loss: 5.954503059387207\n",
      "Epoch: 60, Batch: 200, D_Loss: 0.1268179565668106, G_Loss: 4.9851202964782715\n",
      "Epoch: 60, Batch: 300, D_Loss: 0.15926270186901093, G_Loss: 5.614866733551025\n",
      "Epoch: 60, Batch: 400, D_Loss: 0.1284836232662201, G_Loss: 5.568010330200195\n",
      "Epoch: 60, Batch: 500, D_Loss: 0.06847374886274338, G_Loss: 5.803285598754883\n",
      "Epoch 61\n",
      "Epoch: 61, Batch: 0, D_Loss: 0.07928696274757385, G_Loss: 5.004918098449707\n",
      "Epoch: 61, Batch: 100, D_Loss: 0.04494372755289078, G_Loss: 4.982202053070068\n",
      "Epoch: 61, Batch: 200, D_Loss: 0.0878138393163681, G_Loss: 5.056620121002197\n",
      "Epoch: 61, Batch: 300, D_Loss: 0.09993085265159607, G_Loss: 5.802310943603516\n",
      "Epoch: 61, Batch: 400, D_Loss: 0.11059658974409103, G_Loss: 6.112043380737305\n",
      "Epoch: 61, Batch: 500, D_Loss: 0.06850960105657578, G_Loss: 6.039524078369141\n",
      "Epoch 62\n",
      "Epoch: 62, Batch: 0, D_Loss: 0.12018148601055145, G_Loss: 5.165860652923584\n",
      "Epoch: 62, Batch: 100, D_Loss: 0.0813002809882164, G_Loss: 5.209622859954834\n",
      "Epoch: 62, Batch: 200, D_Loss: 0.06740710139274597, G_Loss: 5.011892318725586\n",
      "Epoch: 62, Batch: 300, D_Loss: 0.10326492786407471, G_Loss: 5.1620192527771\n",
      "Epoch: 62, Batch: 400, D_Loss: 0.09710448980331421, G_Loss: 5.885427951812744\n",
      "Epoch: 62, Batch: 500, D_Loss: 0.08697673678398132, G_Loss: 5.718186855316162\n",
      "Epoch 63\n",
      "Epoch: 63, Batch: 0, D_Loss: 0.2136632800102234, G_Loss: 5.448690414428711\n",
      "Epoch: 63, Batch: 100, D_Loss: 0.023862522095441818, G_Loss: 6.423623561859131\n",
      "Epoch: 63, Batch: 200, D_Loss: 0.048554301261901855, G_Loss: 5.303237438201904\n",
      "Epoch: 63, Batch: 300, D_Loss: 0.09886398911476135, G_Loss: 5.764665126800537\n",
      "Epoch: 63, Batch: 400, D_Loss: 0.1120581179857254, G_Loss: 5.36292839050293\n",
      "Epoch: 63, Batch: 500, D_Loss: 0.13017097115516663, G_Loss: 6.1703033447265625\n",
      "Epoch 64\n",
      "Epoch: 64, Batch: 0, D_Loss: 0.10269351303577423, G_Loss: 5.621638774871826\n",
      "Epoch: 64, Batch: 100, D_Loss: 0.03265909105539322, G_Loss: 5.343041896820068\n",
      "Epoch: 64, Batch: 200, D_Loss: 0.08018732070922852, G_Loss: 5.608566761016846\n",
      "Epoch: 64, Batch: 300, D_Loss: 0.07546927034854889, G_Loss: 5.480935096740723\n",
      "Epoch: 64, Batch: 400, D_Loss: 0.11592818796634674, G_Loss: 6.0623297691345215\n",
      "Epoch: 64, Batch: 500, D_Loss: 0.08784762024879456, G_Loss: 5.289444446563721\n",
      "Epoch 65\n",
      "Epoch: 65, Batch: 0, D_Loss: 0.11507724225521088, G_Loss: 5.194523811340332\n",
      "Epoch: 65, Batch: 100, D_Loss: 0.047491177916526794, G_Loss: 5.455709457397461\n",
      "Epoch: 65, Batch: 200, D_Loss: 0.0700567290186882, G_Loss: 5.771788120269775\n",
      "Epoch: 65, Batch: 300, D_Loss: 0.11819104850292206, G_Loss: 5.883199691772461\n",
      "Epoch: 65, Batch: 400, D_Loss: 0.11274999380111694, G_Loss: 5.200339317321777\n",
      "Epoch: 65, Batch: 500, D_Loss: 0.1750294715166092, G_Loss: 6.670475482940674\n",
      "Epoch 66\n",
      "Epoch: 66, Batch: 0, D_Loss: 0.1296139359474182, G_Loss: 5.284233570098877\n",
      "Epoch: 66, Batch: 100, D_Loss: 0.04177175462245941, G_Loss: 6.089665412902832\n",
      "Epoch: 66, Batch: 200, D_Loss: 0.031160153448581696, G_Loss: 7.2385029792785645\n",
      "Epoch: 66, Batch: 300, D_Loss: 0.3692127466201782, G_Loss: 4.449233055114746\n",
      "Epoch: 66, Batch: 400, D_Loss: 0.09967094659805298, G_Loss: 4.925710201263428\n",
      "Epoch: 66, Batch: 500, D_Loss: 0.2560219168663025, G_Loss: 5.520525932312012\n",
      "Epoch 67\n",
      "Epoch: 67, Batch: 0, D_Loss: 0.03963865339756012, G_Loss: 7.630319118499756\n",
      "Epoch: 67, Batch: 100, D_Loss: 0.05751395225524902, G_Loss: 5.55294942855835\n",
      "Epoch: 67, Batch: 200, D_Loss: 0.035117678344249725, G_Loss: 6.760936260223389\n",
      "Epoch: 67, Batch: 300, D_Loss: 0.022594694048166275, G_Loss: 7.601794242858887\n",
      "Epoch: 67, Batch: 400, D_Loss: 0.040879979729652405, G_Loss: 6.878091812133789\n",
      "Epoch: 67, Batch: 500, D_Loss: 0.07527077943086624, G_Loss: 7.32871150970459\n",
      "Epoch 68\n",
      "Epoch: 68, Batch: 0, D_Loss: 0.12288013100624084, G_Loss: 5.581634044647217\n",
      "Epoch: 68, Batch: 100, D_Loss: 0.03303411602973938, G_Loss: 6.023457050323486\n",
      "Epoch: 68, Batch: 200, D_Loss: 0.09327810257673264, G_Loss: 6.279165267944336\n",
      "Epoch: 68, Batch: 300, D_Loss: 0.11022692173719406, G_Loss: 5.5187835693359375\n",
      "Epoch: 68, Batch: 400, D_Loss: 0.11884230375289917, G_Loss: 5.397762298583984\n",
      "Epoch: 68, Batch: 500, D_Loss: 0.055007874965667725, G_Loss: 6.642645359039307\n",
      "Epoch 69\n",
      "Epoch: 69, Batch: 0, D_Loss: 0.12741509079933167, G_Loss: 5.803840160369873\n",
      "Epoch: 69, Batch: 100, D_Loss: 0.22488804161548615, G_Loss: 4.59574031829834\n",
      "Epoch: 69, Batch: 200, D_Loss: 0.13316410779953003, G_Loss: 5.364687442779541\n",
      "Epoch: 69, Batch: 300, D_Loss: 0.10831130295991898, G_Loss: 5.393744945526123\n",
      "Epoch: 69, Batch: 400, D_Loss: 0.13161058723926544, G_Loss: 5.206246852874756\n",
      "Epoch: 69, Batch: 500, D_Loss: 0.053617507219314575, G_Loss: 5.821331977844238\n",
      "Epoch 70\n",
      "Epoch: 70, Batch: 0, D_Loss: 0.08648685365915298, G_Loss: 5.7725138664245605\n",
      "Epoch: 70, Batch: 100, D_Loss: 0.07067781686782837, G_Loss: 5.370230674743652\n",
      "Epoch: 70, Batch: 200, D_Loss: 0.15731525421142578, G_Loss: 4.999421119689941\n",
      "Epoch: 70, Batch: 300, D_Loss: 0.11844147741794586, G_Loss: 5.244752883911133\n",
      "Epoch: 70, Batch: 400, D_Loss: 0.13218986988067627, G_Loss: 5.140916347503662\n",
      "Epoch: 70, Batch: 500, D_Loss: 0.08403288573026657, G_Loss: 6.23402214050293\n",
      "Epoch 71\n",
      "Epoch: 71, Batch: 0, D_Loss: 0.11832794547080994, G_Loss: 5.78913688659668\n",
      "Epoch: 71, Batch: 100, D_Loss: 0.10214863717556, G_Loss: 4.720062732696533\n",
      "Epoch: 71, Batch: 200, D_Loss: 0.3334147334098816, G_Loss: 5.341842174530029\n",
      "Epoch: 71, Batch: 300, D_Loss: 0.06465959548950195, G_Loss: 5.920700550079346\n",
      "Epoch: 71, Batch: 400, D_Loss: 0.027762720361351967, G_Loss: 6.901554584503174\n",
      "Epoch: 71, Batch: 500, D_Loss: 0.12069445848464966, G_Loss: 7.1808576583862305\n",
      "Epoch 72\n",
      "Epoch: 72, Batch: 0, D_Loss: 0.23997312784194946, G_Loss: 6.007426738739014\n",
      "Epoch: 72, Batch: 100, D_Loss: 0.07093233615159988, G_Loss: 5.314784049987793\n",
      "Epoch: 72, Batch: 200, D_Loss: 0.04524577036499977, G_Loss: 7.044939994812012\n",
      "Epoch: 72, Batch: 300, D_Loss: 0.035266369581222534, G_Loss: 6.550922393798828\n",
      "Epoch: 72, Batch: 400, D_Loss: 0.04641696810722351, G_Loss: 5.317113876342773\n",
      "Epoch: 72, Batch: 500, D_Loss: 0.2592971920967102, G_Loss: 4.882650375366211\n",
      "Epoch 73\n",
      "Epoch: 73, Batch: 0, D_Loss: 0.18188564479351044, G_Loss: 6.249680995941162\n",
      "Epoch: 73, Batch: 100, D_Loss: 0.03551919013261795, G_Loss: 5.7065110206604\n",
      "Epoch: 73, Batch: 200, D_Loss: 0.046289607882499695, G_Loss: 5.952577114105225\n",
      "Epoch: 73, Batch: 300, D_Loss: 0.07778866589069366, G_Loss: 6.104730129241943\n",
      "Epoch: 73, Batch: 400, D_Loss: 0.10214598476886749, G_Loss: 5.312737464904785\n",
      "Epoch: 73, Batch: 500, D_Loss: 0.06881091743707657, G_Loss: 6.033934116363525\n",
      "Epoch 74\n",
      "Epoch: 74, Batch: 0, D_Loss: 0.15901422500610352, G_Loss: 6.097536563873291\n",
      "Epoch: 74, Batch: 100, D_Loss: 0.06878626346588135, G_Loss: 5.111908912658691\n",
      "Epoch: 74, Batch: 200, D_Loss: 0.09154573082923889, G_Loss: 5.620987415313721\n",
      "Epoch: 74, Batch: 300, D_Loss: 0.08314245939254761, G_Loss: 5.39482307434082\n",
      "Epoch: 74, Batch: 400, D_Loss: 0.07940981537103653, G_Loss: 5.028525352478027\n",
      "Epoch: 74, Batch: 500, D_Loss: 0.1345931887626648, G_Loss: 6.175957202911377\n",
      "Epoch 75\n",
      "Epoch: 75, Batch: 0, D_Loss: 0.10208404064178467, G_Loss: 5.407610893249512\n",
      "Epoch: 75, Batch: 100, D_Loss: 0.05404773727059364, G_Loss: 5.349920749664307\n",
      "Epoch: 75, Batch: 200, D_Loss: 0.07953920215368271, G_Loss: 5.884170055389404\n",
      "Epoch: 75, Batch: 300, D_Loss: 0.06391075998544693, G_Loss: 5.385570049285889\n",
      "Epoch: 75, Batch: 400, D_Loss: 0.08351997286081314, G_Loss: 4.9299187660217285\n",
      "Epoch: 75, Batch: 500, D_Loss: 0.07180929183959961, G_Loss: 6.025936126708984\n",
      "Epoch 76\n",
      "Epoch: 76, Batch: 0, D_Loss: 0.15465262532234192, G_Loss: 5.341789722442627\n",
      "Epoch: 76, Batch: 100, D_Loss: 0.08270002156496048, G_Loss: 5.234513759613037\n",
      "Epoch: 76, Batch: 200, D_Loss: 0.1219983845949173, G_Loss: 4.903652191162109\n",
      "Epoch: 76, Batch: 300, D_Loss: 0.11004067957401276, G_Loss: 5.617169380187988\n",
      "Epoch: 76, Batch: 400, D_Loss: 0.011286288499832153, G_Loss: 7.014408588409424\n",
      "Epoch: 76, Batch: 500, D_Loss: 0.0653098002076149, G_Loss: 5.420236587524414\n",
      "Epoch 77\n",
      "Epoch: 77, Batch: 0, D_Loss: 0.07676683366298676, G_Loss: 5.81348991394043\n",
      "Epoch: 77, Batch: 100, D_Loss: 0.11555841565132141, G_Loss: 4.9397687911987305\n",
      "Epoch: 77, Batch: 200, D_Loss: 0.08929738402366638, G_Loss: 5.894492149353027\n",
      "Epoch: 77, Batch: 300, D_Loss: 0.12328057736158371, G_Loss: 5.090359687805176\n",
      "Epoch: 77, Batch: 400, D_Loss: 0.15875042974948883, G_Loss: 6.735466480255127\n",
      "Epoch: 77, Batch: 500, D_Loss: 0.09214502573013306, G_Loss: 6.269284725189209\n",
      "Epoch 78\n",
      "Epoch: 78, Batch: 0, D_Loss: 0.07041803747415543, G_Loss: 5.887872219085693\n",
      "Epoch: 78, Batch: 100, D_Loss: 0.02004006877541542, G_Loss: 6.78155517578125\n",
      "Epoch: 78, Batch: 200, D_Loss: 0.11446311324834824, G_Loss: 5.360486030578613\n",
      "Epoch: 78, Batch: 300, D_Loss: 0.016561340540647507, G_Loss: 7.175379753112793\n",
      "Epoch: 78, Batch: 400, D_Loss: 0.003468146314844489, G_Loss: 8.056534767150879\n",
      "Epoch: 78, Batch: 500, D_Loss: 0.036955270916223526, G_Loss: 6.711260795593262\n",
      "Epoch 79\n",
      "Epoch: 79, Batch: 0, D_Loss: 0.060287076979875565, G_Loss: 7.180999279022217\n",
      "Epoch: 79, Batch: 100, D_Loss: 0.4319536089897156, G_Loss: 3.7255773544311523\n",
      "Epoch: 79, Batch: 200, D_Loss: 0.23853622376918793, G_Loss: 5.166813373565674\n",
      "Epoch: 79, Batch: 300, D_Loss: 0.038625769317150116, G_Loss: 6.7786102294921875\n",
      "Epoch: 79, Batch: 400, D_Loss: 0.01948251947760582, G_Loss: 7.830428600311279\n",
      "Epoch: 79, Batch: 500, D_Loss: 0.022491589188575745, G_Loss: 7.871303558349609\n",
      "Epoch 80\n",
      "Epoch: 80, Batch: 0, D_Loss: 0.1166364923119545, G_Loss: 5.776068687438965\n",
      "Epoch: 80, Batch: 100, D_Loss: 0.07809291779994965, G_Loss: 5.112863063812256\n",
      "Epoch: 80, Batch: 200, D_Loss: 0.19330984354019165, G_Loss: 5.026961803436279\n",
      "Epoch: 80, Batch: 300, D_Loss: 0.16360199451446533, G_Loss: 6.133139610290527\n",
      "Epoch: 80, Batch: 400, D_Loss: 0.07693443447351456, G_Loss: 6.110316276550293\n",
      "Epoch: 80, Batch: 500, D_Loss: 0.12287956476211548, G_Loss: 7.304068565368652\n",
      "Epoch 81\n",
      "Epoch: 81, Batch: 0, D_Loss: 0.014162182807922363, G_Loss: 7.4350457191467285\n",
      "Epoch: 81, Batch: 100, D_Loss: 0.04583311825990677, G_Loss: 6.583378314971924\n",
      "Epoch: 81, Batch: 200, D_Loss: 0.14549295604228973, G_Loss: 5.4521589279174805\n",
      "Epoch: 81, Batch: 300, D_Loss: 0.0897810310125351, G_Loss: 4.765884876251221\n",
      "Epoch: 81, Batch: 400, D_Loss: 0.34439873695373535, G_Loss: 5.357077598571777\n",
      "Epoch: 81, Batch: 500, D_Loss: 0.1054362878203392, G_Loss: 6.072854518890381\n",
      "Epoch 82\n",
      "Epoch: 82, Batch: 0, D_Loss: 0.06410486251115799, G_Loss: 6.698640823364258\n",
      "Epoch: 82, Batch: 100, D_Loss: 0.012190952897071838, G_Loss: 6.685211181640625\n",
      "Epoch: 82, Batch: 200, D_Loss: 0.07494981586933136, G_Loss: 5.931967258453369\n",
      "Epoch: 82, Batch: 300, D_Loss: 0.10296015441417694, G_Loss: 6.560642242431641\n",
      "Epoch: 82, Batch: 400, D_Loss: 0.18127167224884033, G_Loss: 4.5806403160095215\n",
      "Epoch: 82, Batch: 500, D_Loss: 0.11334167420864105, G_Loss: 6.095058441162109\n",
      "Epoch 83\n",
      "Epoch: 83, Batch: 0, D_Loss: 0.1331399530172348, G_Loss: 3.9346611499786377\n",
      "Epoch: 83, Batch: 100, D_Loss: 0.07424705475568771, G_Loss: 5.250013828277588\n",
      "Epoch: 83, Batch: 200, D_Loss: 0.09756194800138474, G_Loss: 4.718097686767578\n",
      "Epoch: 83, Batch: 300, D_Loss: 0.10167951881885529, G_Loss: 5.403721332550049\n",
      "Epoch: 83, Batch: 400, D_Loss: 0.10314013063907623, G_Loss: 5.088020324707031\n",
      "Epoch: 83, Batch: 500, D_Loss: 0.1379171460866928, G_Loss: 5.856601715087891\n",
      "Epoch 84\n",
      "Epoch: 84, Batch: 0, D_Loss: 0.08018794655799866, G_Loss: 4.961050033569336\n",
      "Epoch: 84, Batch: 100, D_Loss: 0.062360189855098724, G_Loss: 5.236703872680664\n",
      "Epoch: 84, Batch: 200, D_Loss: 0.10453331470489502, G_Loss: 5.295019626617432\n",
      "Epoch: 84, Batch: 300, D_Loss: 0.11135600507259369, G_Loss: 4.8750104904174805\n",
      "Epoch: 84, Batch: 400, D_Loss: 0.11488085240125656, G_Loss: 4.710413932800293\n",
      "Epoch: 84, Batch: 500, D_Loss: 0.44042420387268066, G_Loss: 5.575798511505127\n",
      "Epoch 85\n",
      "Epoch: 85, Batch: 0, D_Loss: 0.4905019700527191, G_Loss: 5.419591903686523\n",
      "Epoch: 85, Batch: 100, D_Loss: 0.12144913524389267, G_Loss: 4.903977870941162\n",
      "Epoch: 85, Batch: 200, D_Loss: 0.06687408685684204, G_Loss: 5.294053554534912\n",
      "Epoch: 85, Batch: 300, D_Loss: 0.04569613188505173, G_Loss: 5.800590991973877\n",
      "Epoch: 85, Batch: 400, D_Loss: 0.02368325926363468, G_Loss: 6.493259429931641\n",
      "Epoch: 85, Batch: 500, D_Loss: 0.047243960201740265, G_Loss: 6.254528999328613\n",
      "Epoch 86\n",
      "Epoch: 86, Batch: 0, D_Loss: 0.11642874777317047, G_Loss: 5.138550281524658\n",
      "Epoch: 86, Batch: 100, D_Loss: 0.07827979326248169, G_Loss: 5.670586109161377\n",
      "Epoch: 86, Batch: 200, D_Loss: 0.17398932576179504, G_Loss: 4.8537373542785645\n",
      "Epoch: 86, Batch: 300, D_Loss: 0.09715928137302399, G_Loss: 6.010255336761475\n",
      "Epoch: 86, Batch: 400, D_Loss: 0.14749902486801147, G_Loss: 4.436796188354492\n",
      "Epoch: 86, Batch: 500, D_Loss: 0.07189995050430298, G_Loss: 6.560780048370361\n",
      "Epoch 87\n",
      "Epoch: 87, Batch: 0, D_Loss: 0.07957618683576584, G_Loss: 6.1796464920043945\n",
      "Epoch: 87, Batch: 100, D_Loss: 0.05402747541666031, G_Loss: 6.07228946685791\n",
      "Epoch: 87, Batch: 200, D_Loss: 0.11357083916664124, G_Loss: 6.203092098236084\n",
      "Epoch: 87, Batch: 300, D_Loss: 0.14448729157447815, G_Loss: 5.503697395324707\n",
      "Epoch: 87, Batch: 400, D_Loss: 0.2691841721534729, G_Loss: 4.6545491218566895\n",
      "Epoch: 87, Batch: 500, D_Loss: 0.11366832256317139, G_Loss: 6.032695770263672\n",
      "Epoch 88\n",
      "Epoch: 88, Batch: 0, D_Loss: 0.0905870869755745, G_Loss: 6.2978835105896\n",
      "Epoch: 88, Batch: 100, D_Loss: 0.07575908303260803, G_Loss: 5.792517185211182\n",
      "Epoch: 88, Batch: 200, D_Loss: 0.04114614427089691, G_Loss: 6.617074012756348\n",
      "Epoch: 88, Batch: 300, D_Loss: 0.07868167012929916, G_Loss: 6.285823345184326\n",
      "Epoch: 88, Batch: 400, D_Loss: 0.09364475309848785, G_Loss: 6.016433238983154\n",
      "Epoch: 88, Batch: 500, D_Loss: 0.12667977809906006, G_Loss: 5.643332004547119\n",
      "Epoch 89\n",
      "Epoch: 89, Batch: 0, D_Loss: 0.10754094272851944, G_Loss: 5.554002285003662\n",
      "Epoch: 89, Batch: 100, D_Loss: 0.05172371119260788, G_Loss: 5.790775299072266\n",
      "Epoch: 89, Batch: 200, D_Loss: 0.05704544857144356, G_Loss: 6.151698112487793\n",
      "Epoch: 89, Batch: 300, D_Loss: 0.07082357257604599, G_Loss: 6.587550163269043\n",
      "Epoch: 89, Batch: 400, D_Loss: 0.11414147168397903, G_Loss: 4.9995222091674805\n",
      "Epoch: 89, Batch: 500, D_Loss: 0.20963358879089355, G_Loss: 5.151080131530762\n",
      "Epoch 90\n",
      "Epoch: 90, Batch: 0, D_Loss: 0.1371147483587265, G_Loss: 5.150673866271973\n",
      "Epoch: 90, Batch: 100, D_Loss: 0.09288148581981659, G_Loss: 4.873044013977051\n",
      "Epoch: 90, Batch: 200, D_Loss: 0.15155726671218872, G_Loss: 4.843112945556641\n",
      "Epoch: 90, Batch: 300, D_Loss: 0.055662862956523895, G_Loss: 5.604976177215576\n",
      "Epoch: 90, Batch: 400, D_Loss: 0.09903046488761902, G_Loss: 4.816591739654541\n",
      "Epoch: 90, Batch: 500, D_Loss: 0.12971170246601105, G_Loss: 5.533612251281738\n",
      "Epoch 91\n",
      "Epoch: 91, Batch: 0, D_Loss: 0.15796324610710144, G_Loss: 4.816070556640625\n",
      "Epoch: 91, Batch: 100, D_Loss: 0.05069243907928467, G_Loss: 5.763630390167236\n",
      "Epoch: 91, Batch: 200, D_Loss: 0.08608690649271011, G_Loss: 5.203936576843262\n",
      "Epoch: 91, Batch: 300, D_Loss: 0.15650692582130432, G_Loss: 4.985626697540283\n",
      "Epoch: 91, Batch: 400, D_Loss: 0.10638092458248138, G_Loss: 5.334469795227051\n",
      "Epoch: 91, Batch: 500, D_Loss: 0.13734352588653564, G_Loss: 5.517898082733154\n",
      "Epoch 92\n",
      "Epoch: 92, Batch: 0, D_Loss: 0.09514743089675903, G_Loss: 4.635936737060547\n",
      "Epoch: 92, Batch: 100, D_Loss: 0.06311963498592377, G_Loss: 5.531255722045898\n",
      "Epoch: 92, Batch: 200, D_Loss: 0.16788101196289062, G_Loss: 4.836538791656494\n",
      "Epoch: 92, Batch: 300, D_Loss: 0.06296590715646744, G_Loss: 5.485837459564209\n",
      "Epoch: 92, Batch: 400, D_Loss: 0.05759640783071518, G_Loss: 5.253790378570557\n",
      "Epoch: 92, Batch: 500, D_Loss: 0.11262141168117523, G_Loss: 4.745434284210205\n",
      "Epoch 93\n",
      "Epoch: 93, Batch: 0, D_Loss: 0.11703447997570038, G_Loss: 4.975428581237793\n",
      "Epoch: 93, Batch: 100, D_Loss: 0.06773783266544342, G_Loss: 4.749385356903076\n",
      "Epoch: 93, Batch: 200, D_Loss: 0.16842582821846008, G_Loss: 4.497435092926025\n",
      "Epoch: 93, Batch: 300, D_Loss: 0.07539595663547516, G_Loss: 4.9792070388793945\n",
      "Epoch: 93, Batch: 400, D_Loss: 0.11392567306756973, G_Loss: 4.277109146118164\n",
      "Epoch: 93, Batch: 500, D_Loss: 0.16359572112560272, G_Loss: 5.402867317199707\n",
      "Epoch 94\n",
      "Epoch: 94, Batch: 0, D_Loss: 0.06428691744804382, G_Loss: 5.744304656982422\n",
      "Epoch: 94, Batch: 100, D_Loss: 0.014516843482851982, G_Loss: 5.984370708465576\n",
      "Epoch: 94, Batch: 200, D_Loss: 0.02533211186528206, G_Loss: 6.354801177978516\n",
      "Epoch: 94, Batch: 300, D_Loss: 0.08711905777454376, G_Loss: 5.408181190490723\n",
      "Epoch: 94, Batch: 400, D_Loss: 0.03887355327606201, G_Loss: 5.583154678344727\n",
      "Epoch: 94, Batch: 500, D_Loss: 0.08234725892543793, G_Loss: 5.931523323059082\n",
      "Epoch 95\n",
      "Epoch: 95, Batch: 0, D_Loss: 0.059636667370796204, G_Loss: 6.388343334197998\n",
      "Epoch: 95, Batch: 100, D_Loss: 0.09203612804412842, G_Loss: 5.098188400268555\n",
      "Epoch: 95, Batch: 200, D_Loss: 0.17892374098300934, G_Loss: 4.9309258460998535\n",
      "Epoch: 95, Batch: 300, D_Loss: 0.1204291582107544, G_Loss: 5.177082061767578\n",
      "Epoch: 95, Batch: 400, D_Loss: 0.10540260374546051, G_Loss: 4.582934379577637\n",
      "Epoch: 95, Batch: 500, D_Loss: 0.11215850710868835, G_Loss: 4.96260929107666\n",
      "Epoch 96\n",
      "Epoch: 96, Batch: 0, D_Loss: 0.07262050360441208, G_Loss: 5.1917724609375\n",
      "Epoch: 96, Batch: 100, D_Loss: 0.092962846159935, G_Loss: 4.815427303314209\n",
      "Epoch: 96, Batch: 200, D_Loss: 0.10520490258932114, G_Loss: 4.828891754150391\n",
      "Epoch: 96, Batch: 300, D_Loss: 0.17532978951931, G_Loss: 4.656635761260986\n",
      "Epoch: 96, Batch: 400, D_Loss: 0.14555110037326813, G_Loss: 4.071817398071289\n",
      "Epoch: 96, Batch: 500, D_Loss: 0.14160564541816711, G_Loss: 5.340133190155029\n",
      "Epoch 97\n",
      "Epoch: 97, Batch: 0, D_Loss: 0.07841821759939194, G_Loss: 5.665118217468262\n",
      "Epoch: 97, Batch: 100, D_Loss: 0.08591259270906448, G_Loss: 5.199075698852539\n",
      "Epoch: 97, Batch: 200, D_Loss: 0.058808498084545135, G_Loss: 5.861346244812012\n",
      "Epoch: 97, Batch: 300, D_Loss: 0.08536563068628311, G_Loss: 4.888822078704834\n",
      "Epoch: 97, Batch: 400, D_Loss: 0.10381349921226501, G_Loss: 4.797291278839111\n",
      "Epoch: 97, Batch: 500, D_Loss: 0.10978955775499344, G_Loss: 5.554055690765381\n",
      "Epoch 98\n",
      "Epoch: 98, Batch: 0, D_Loss: 0.08744364231824875, G_Loss: 4.54024076461792\n",
      "Epoch: 98, Batch: 100, D_Loss: 0.13633586466312408, G_Loss: 4.1881279945373535\n",
      "Epoch: 98, Batch: 200, D_Loss: 0.11295145750045776, G_Loss: 4.584420680999756\n",
      "Epoch: 98, Batch: 300, D_Loss: 0.21828433871269226, G_Loss: 4.677030086517334\n",
      "Epoch: 98, Batch: 400, D_Loss: 0.15730583667755127, G_Loss: 4.027078151702881\n",
      "Epoch: 98, Batch: 500, D_Loss: 0.10489404201507568, G_Loss: 5.158830165863037\n",
      "Epoch 99\n",
      "Epoch: 99, Batch: 0, D_Loss: 0.11745940893888474, G_Loss: 5.175888538360596\n",
      "Epoch: 99, Batch: 100, D_Loss: 0.09364758431911469, G_Loss: 4.546082019805908\n",
      "Epoch: 99, Batch: 200, D_Loss: 0.1288437694311142, G_Loss: 5.069077014923096\n",
      "Epoch: 99, Batch: 300, D_Loss: 0.11542990058660507, G_Loss: 4.962681293487549\n",
      "Epoch: 99, Batch: 400, D_Loss: 0.13107949495315552, G_Loss: 4.848942756652832\n",
      "Epoch: 99, Batch: 500, D_Loss: 0.11677974462509155, G_Loss: 4.962578773498535\n",
      "Epoch 100\n",
      "Epoch: 100, Batch: 0, D_Loss: 0.09097093343734741, G_Loss: 5.345645904541016\n",
      "Epoch: 100, Batch: 100, D_Loss: 0.09792102873325348, G_Loss: 4.599518299102783\n",
      "Epoch: 100, Batch: 200, D_Loss: 0.15165093541145325, G_Loss: 4.344969272613525\n",
      "Epoch: 100, Batch: 300, D_Loss: 0.1521376520395279, G_Loss: 4.239854335784912\n",
      "Epoch: 100, Batch: 400, D_Loss: 0.12636685371398926, G_Loss: 4.72596549987793\n",
      "Epoch: 100, Batch: 500, D_Loss: 0.09944016486406326, G_Loss: 5.167003631591797\n",
      "Epoch 101\n",
      "Epoch: 101, Batch: 0, D_Loss: 0.06393015384674072, G_Loss: 5.58551025390625\n",
      "Epoch: 101, Batch: 100, D_Loss: 0.08618856966495514, G_Loss: 4.778249740600586\n",
      "Epoch: 101, Batch: 200, D_Loss: 0.13931801915168762, G_Loss: 5.327221870422363\n",
      "Epoch: 101, Batch: 300, D_Loss: 0.12555518746376038, G_Loss: 4.5118608474731445\n",
      "Epoch: 101, Batch: 400, D_Loss: 0.1241282969713211, G_Loss: 4.563121795654297\n",
      "Epoch: 101, Batch: 500, D_Loss: 0.08270089328289032, G_Loss: 4.945875644683838\n",
      "Epoch 102\n",
      "Epoch: 102, Batch: 0, D_Loss: 0.11429968476295471, G_Loss: 4.820960521697998\n",
      "Epoch: 102, Batch: 100, D_Loss: 0.05891356244683266, G_Loss: 4.493642807006836\n",
      "Epoch: 102, Batch: 200, D_Loss: 0.1568876951932907, G_Loss: 4.847365379333496\n",
      "Epoch: 102, Batch: 300, D_Loss: 0.11210426688194275, G_Loss: 4.808697700500488\n",
      "Epoch: 102, Batch: 400, D_Loss: 0.11602631956338882, G_Loss: 4.440529823303223\n",
      "Epoch: 102, Batch: 500, D_Loss: 0.07666555047035217, G_Loss: 4.904959678649902\n",
      "Epoch 103\n",
      "Epoch: 103, Batch: 0, D_Loss: 0.09890665858983994, G_Loss: 4.8291401863098145\n",
      "Epoch: 103, Batch: 100, D_Loss: 0.08261570334434509, G_Loss: 4.6342549324035645\n",
      "Epoch: 103, Batch: 200, D_Loss: 0.14525410532951355, G_Loss: 4.953596591949463\n",
      "Epoch: 103, Batch: 300, D_Loss: 0.15028652548789978, G_Loss: 4.8029375076293945\n",
      "Epoch: 103, Batch: 400, D_Loss: 0.23620399832725525, G_Loss: 3.8739492893218994\n",
      "Epoch: 103, Batch: 500, D_Loss: 0.07690518349409103, G_Loss: 5.517808437347412\n",
      "Epoch 104\n",
      "Epoch: 104, Batch: 0, D_Loss: 0.10175295174121857, G_Loss: 4.503089427947998\n",
      "Epoch: 104, Batch: 100, D_Loss: 0.15858346223831177, G_Loss: 4.3515214920043945\n",
      "Epoch: 104, Batch: 200, D_Loss: 0.14736433327198029, G_Loss: 4.341352462768555\n",
      "Epoch: 104, Batch: 300, D_Loss: 0.1397407054901123, G_Loss: 4.6609296798706055\n",
      "Epoch: 104, Batch: 400, D_Loss: 0.17186133563518524, G_Loss: 4.225917816162109\n",
      "Epoch: 104, Batch: 500, D_Loss: 0.08329179883003235, G_Loss: 5.388241767883301\n",
      "Epoch 105\n",
      "Epoch: 105, Batch: 0, D_Loss: 0.12344075739383698, G_Loss: 4.274068832397461\n",
      "Epoch: 105, Batch: 100, D_Loss: 0.07963425666093826, G_Loss: 4.467329025268555\n",
      "Epoch: 105, Batch: 200, D_Loss: 0.1453113853931427, G_Loss: 5.1445441246032715\n",
      "Epoch: 105, Batch: 300, D_Loss: 0.15759488940238953, G_Loss: 4.793075084686279\n",
      "Epoch: 105, Batch: 400, D_Loss: 0.15099909901618958, G_Loss: 4.302953720092773\n",
      "Epoch: 105, Batch: 500, D_Loss: 0.07220179587602615, G_Loss: 5.128499984741211\n",
      "Epoch 106\n",
      "Epoch: 106, Batch: 0, D_Loss: 0.1480802744626999, G_Loss: 4.032852649688721\n",
      "Epoch: 106, Batch: 100, D_Loss: 0.07251358777284622, G_Loss: 4.344876766204834\n",
      "Epoch: 106, Batch: 200, D_Loss: 0.1758170872926712, G_Loss: 4.370881080627441\n",
      "Epoch: 106, Batch: 300, D_Loss: 0.13452039659023285, G_Loss: 4.861788272857666\n",
      "Epoch: 106, Batch: 400, D_Loss: 0.09538399428129196, G_Loss: 3.8527820110321045\n",
      "Epoch: 106, Batch: 500, D_Loss: 0.10075315833091736, G_Loss: 5.035233497619629\n",
      "Epoch 107\n",
      "Epoch: 107, Batch: 0, D_Loss: 0.14476151764392853, G_Loss: 4.384243965148926\n",
      "Epoch: 107, Batch: 100, D_Loss: 0.04682803899049759, G_Loss: 5.248058319091797\n",
      "Epoch: 107, Batch: 200, D_Loss: 0.15261191129684448, G_Loss: 4.963953971862793\n",
      "Epoch: 107, Batch: 300, D_Loss: 0.11958447098731995, G_Loss: 4.4296793937683105\n",
      "Epoch: 107, Batch: 400, D_Loss: 0.11887186020612717, G_Loss: 4.672555923461914\n",
      "Epoch: 107, Batch: 500, D_Loss: 0.06906858086585999, G_Loss: 5.03132963180542\n",
      "Epoch 108\n",
      "Epoch: 108, Batch: 0, D_Loss: 0.07975269854068756, G_Loss: 4.471620559692383\n",
      "Epoch: 108, Batch: 100, D_Loss: 0.08212913572788239, G_Loss: 4.396543025970459\n",
      "Epoch: 108, Batch: 200, D_Loss: 0.1641061156988144, G_Loss: 5.665561676025391\n",
      "Epoch: 108, Batch: 300, D_Loss: 0.14904779195785522, G_Loss: 4.77073860168457\n",
      "Epoch: 108, Batch: 400, D_Loss: 0.0975455641746521, G_Loss: 4.6953277587890625\n",
      "Epoch: 108, Batch: 500, D_Loss: 0.05682025104761124, G_Loss: 5.636430740356445\n",
      "Epoch 109\n",
      "Epoch: 109, Batch: 0, D_Loss: 0.1326926350593567, G_Loss: 4.431032657623291\n",
      "Epoch: 109, Batch: 100, D_Loss: 0.0699397474527359, G_Loss: 4.48472261428833\n",
      "Epoch: 109, Batch: 200, D_Loss: 0.10917357355356216, G_Loss: 4.553156852722168\n",
      "Epoch: 109, Batch: 300, D_Loss: 0.08429288864135742, G_Loss: 5.229583740234375\n",
      "Epoch: 109, Batch: 400, D_Loss: 0.12436886131763458, G_Loss: 4.130868434906006\n",
      "Epoch: 109, Batch: 500, D_Loss: 0.09037646651268005, G_Loss: 4.8727030754089355\n",
      "Epoch 110\n",
      "Epoch: 110, Batch: 0, D_Loss: 0.07760963588953018, G_Loss: 4.408132553100586\n",
      "Epoch: 110, Batch: 100, D_Loss: 0.0958549976348877, G_Loss: 4.489686965942383\n",
      "Epoch: 110, Batch: 200, D_Loss: 0.17542880773544312, G_Loss: 4.671022415161133\n",
      "Epoch: 110, Batch: 300, D_Loss: 0.1723785698413849, G_Loss: 4.534799575805664\n",
      "Epoch: 110, Batch: 400, D_Loss: 0.12815389037132263, G_Loss: 4.318423271179199\n",
      "Epoch: 110, Batch: 500, D_Loss: 0.10806257277727127, G_Loss: 4.5380730628967285\n",
      "Epoch 111\n",
      "Epoch: 111, Batch: 0, D_Loss: 0.07313152402639389, G_Loss: 4.906005382537842\n",
      "Epoch: 111, Batch: 100, D_Loss: 0.09137845784425735, G_Loss: 4.549352169036865\n",
      "Epoch: 111, Batch: 200, D_Loss: 0.1562381535768509, G_Loss: 4.913456916809082\n",
      "Epoch: 111, Batch: 300, D_Loss: 0.1070219874382019, G_Loss: 4.764348030090332\n",
      "Epoch: 111, Batch: 400, D_Loss: 0.15954512357711792, G_Loss: 4.364780426025391\n",
      "Epoch: 111, Batch: 500, D_Loss: 0.08073504269123077, G_Loss: 4.901587009429932\n",
      "Epoch 112\n",
      "Epoch: 112, Batch: 0, D_Loss: 0.10752274096012115, G_Loss: 4.564034938812256\n",
      "Epoch: 112, Batch: 100, D_Loss: 0.09548759460449219, G_Loss: 3.777987003326416\n",
      "Epoch: 112, Batch: 200, D_Loss: 0.1286754012107849, G_Loss: 4.233348369598389\n",
      "Epoch: 112, Batch: 300, D_Loss: 0.0981459766626358, G_Loss: 4.110589981079102\n",
      "Epoch: 112, Batch: 400, D_Loss: 0.22397789359092712, G_Loss: 3.8562560081481934\n",
      "Epoch: 112, Batch: 500, D_Loss: 0.05530441552400589, G_Loss: 5.090685844421387\n",
      "Epoch 113\n",
      "Epoch: 113, Batch: 0, D_Loss: 0.07620520889759064, G_Loss: 4.743617057800293\n",
      "Epoch: 113, Batch: 100, D_Loss: 0.06575021147727966, G_Loss: 4.019228935241699\n",
      "Epoch: 113, Batch: 200, D_Loss: 0.1313886046409607, G_Loss: 4.257293701171875\n",
      "Epoch: 113, Batch: 300, D_Loss: 0.10270263254642487, G_Loss: 4.240494251251221\n",
      "Epoch: 113, Batch: 400, D_Loss: 0.12278734147548676, G_Loss: 4.836615562438965\n",
      "Epoch: 113, Batch: 500, D_Loss: 0.05442139878869057, G_Loss: 6.070425987243652\n",
      "Epoch 114\n",
      "Epoch: 114, Batch: 0, D_Loss: 0.14033284783363342, G_Loss: 4.283053398132324\n",
      "Epoch: 114, Batch: 100, D_Loss: 0.11191629618406296, G_Loss: 4.199316501617432\n",
      "Epoch: 114, Batch: 200, D_Loss: 0.1410335898399353, G_Loss: 4.3931193351745605\n",
      "Epoch: 114, Batch: 300, D_Loss: 0.18330654501914978, G_Loss: 4.027316570281982\n",
      "Epoch: 114, Batch: 400, D_Loss: 0.1049230769276619, G_Loss: 4.007800102233887\n",
      "Epoch: 114, Batch: 500, D_Loss: 0.12648586928844452, G_Loss: 4.7992658615112305\n",
      "Epoch 115\n",
      "Epoch: 115, Batch: 0, D_Loss: 0.0791335478425026, G_Loss: 4.3224945068359375\n",
      "Epoch: 115, Batch: 100, D_Loss: 0.10153232514858246, G_Loss: 4.309514999389648\n",
      "Epoch: 115, Batch: 200, D_Loss: 0.13212060928344727, G_Loss: 4.883741855621338\n",
      "Epoch: 115, Batch: 300, D_Loss: 0.12588858604431152, G_Loss: 4.161805152893066\n",
      "Epoch: 115, Batch: 400, D_Loss: 0.13798551261425018, G_Loss: 4.287746906280518\n",
      "Epoch: 115, Batch: 500, D_Loss: 0.07021121680736542, G_Loss: 4.913280487060547\n",
      "Epoch 116\n",
      "Epoch: 116, Batch: 0, D_Loss: 0.13304434716701508, G_Loss: 3.9093594551086426\n",
      "Epoch: 116, Batch: 100, D_Loss: 0.09389561414718628, G_Loss: 4.940390110015869\n",
      "Epoch: 116, Batch: 200, D_Loss: 0.1489109843969345, G_Loss: 4.921988487243652\n",
      "Epoch: 116, Batch: 300, D_Loss: 0.08443507552146912, G_Loss: 4.812047004699707\n",
      "Epoch: 116, Batch: 400, D_Loss: 0.1863589584827423, G_Loss: 4.1498236656188965\n",
      "Epoch: 116, Batch: 500, D_Loss: 0.08397188037633896, G_Loss: 5.043095588684082\n",
      "Epoch 117\n",
      "Epoch: 117, Batch: 0, D_Loss: 0.08366735279560089, G_Loss: 4.794826507568359\n",
      "Epoch: 117, Batch: 100, D_Loss: 0.07323068380355835, G_Loss: 4.7146501541137695\n",
      "Epoch: 117, Batch: 200, D_Loss: 0.14618049561977386, G_Loss: 4.875085830688477\n",
      "Epoch: 117, Batch: 300, D_Loss: 0.11768025904893875, G_Loss: 4.069479942321777\n",
      "Epoch: 117, Batch: 400, D_Loss: 0.10415640473365784, G_Loss: 4.542279243469238\n",
      "Epoch: 117, Batch: 500, D_Loss: 0.08841660618782043, G_Loss: 5.049501895904541\n",
      "Epoch 118\n",
      "Epoch: 118, Batch: 0, D_Loss: 0.11102787405252457, G_Loss: 4.222911357879639\n",
      "Epoch: 118, Batch: 100, D_Loss: 0.09302225708961487, G_Loss: 4.577921390533447\n",
      "Epoch: 118, Batch: 200, D_Loss: 0.14452621340751648, G_Loss: 4.417855262756348\n",
      "Epoch: 118, Batch: 300, D_Loss: 0.14776086807250977, G_Loss: 4.20866584777832\n",
      "Epoch: 118, Batch: 400, D_Loss: 0.09510117024183273, G_Loss: 4.398083686828613\n",
      "Epoch: 118, Batch: 500, D_Loss: 0.11438470333814621, G_Loss: 4.941500186920166\n",
      "Epoch 119\n",
      "Epoch: 119, Batch: 0, D_Loss: 0.16738083958625793, G_Loss: 4.103631973266602\n",
      "Epoch: 119, Batch: 100, D_Loss: 0.10781222581863403, G_Loss: 5.092434883117676\n",
      "Epoch: 119, Batch: 200, D_Loss: 0.10343154519796371, G_Loss: 5.153848648071289\n",
      "Epoch: 119, Batch: 300, D_Loss: 0.12365438044071198, G_Loss: 4.654360771179199\n",
      "Epoch: 119, Batch: 400, D_Loss: 0.07301954925060272, G_Loss: 4.278341770172119\n",
      "Epoch: 119, Batch: 500, D_Loss: 0.11564960330724716, G_Loss: 4.574931621551514\n",
      "Epoch 120\n",
      "Epoch: 120, Batch: 0, D_Loss: 0.08952133357524872, G_Loss: 4.201765537261963\n",
      "Epoch: 120, Batch: 100, D_Loss: 0.12165341526269913, G_Loss: 5.005291938781738\n",
      "Epoch: 120, Batch: 200, D_Loss: 0.13468682765960693, G_Loss: 4.716839790344238\n",
      "Epoch: 120, Batch: 300, D_Loss: 0.22847414016723633, G_Loss: 3.680560827255249\n",
      "Epoch: 120, Batch: 400, D_Loss: 0.1243625059723854, G_Loss: 4.302440643310547\n",
      "Epoch: 120, Batch: 500, D_Loss: 0.06738201528787613, G_Loss: 5.063520431518555\n",
      "Epoch 121\n",
      "Epoch: 121, Batch: 0, D_Loss: 0.17015251517295837, G_Loss: 4.45106840133667\n",
      "Epoch: 121, Batch: 100, D_Loss: 0.0896935909986496, G_Loss: 4.561993598937988\n",
      "Epoch: 121, Batch: 200, D_Loss: 0.12133176624774933, G_Loss: 4.984496593475342\n",
      "Epoch: 121, Batch: 300, D_Loss: 0.21963268518447876, G_Loss: 4.661719799041748\n",
      "Epoch: 121, Batch: 400, D_Loss: 0.11871690303087234, G_Loss: 4.894411087036133\n",
      "Epoch: 121, Batch: 500, D_Loss: 0.1016932874917984, G_Loss: 5.113967418670654\n",
      "Epoch 122\n",
      "Epoch: 122, Batch: 0, D_Loss: 0.1248970478773117, G_Loss: 4.352571487426758\n",
      "Epoch: 122, Batch: 100, D_Loss: 0.0921800434589386, G_Loss: 4.716873645782471\n",
      "Epoch: 122, Batch: 200, D_Loss: 0.21536915004253387, G_Loss: 4.005307674407959\n",
      "Epoch: 122, Batch: 300, D_Loss: 0.11909151077270508, G_Loss: 4.382101058959961\n",
      "Epoch: 122, Batch: 400, D_Loss: 0.08152629435062408, G_Loss: 5.067816734313965\n",
      "Epoch: 122, Batch: 500, D_Loss: 0.05319353938102722, G_Loss: 5.063165187835693\n",
      "Epoch 123\n",
      "Epoch: 123, Batch: 0, D_Loss: 0.10595570504665375, G_Loss: 4.300596714019775\n",
      "Epoch: 123, Batch: 100, D_Loss: 0.10767245292663574, G_Loss: 4.901891708374023\n",
      "Epoch: 123, Batch: 200, D_Loss: 0.13391423225402832, G_Loss: 4.064812183380127\n",
      "Epoch: 123, Batch: 300, D_Loss: 0.14169283211231232, G_Loss: 4.450425148010254\n",
      "Epoch: 123, Batch: 400, D_Loss: 0.07528994232416153, G_Loss: 4.47264289855957\n",
      "Epoch: 123, Batch: 500, D_Loss: 0.09351211786270142, G_Loss: 5.070704460144043\n",
      "Epoch 124\n",
      "Epoch: 124, Batch: 0, D_Loss: 0.10328316688537598, G_Loss: 4.228106498718262\n",
      "Epoch: 124, Batch: 100, D_Loss: 0.06860774010419846, G_Loss: 5.317813873291016\n",
      "Epoch: 124, Batch: 200, D_Loss: 0.14924544095993042, G_Loss: 4.217467784881592\n",
      "Epoch: 124, Batch: 300, D_Loss: 0.14556224644184113, G_Loss: 4.4699177742004395\n",
      "Epoch: 124, Batch: 400, D_Loss: 0.074413001537323, G_Loss: 5.358747482299805\n",
      "Epoch: 124, Batch: 500, D_Loss: 0.04585704207420349, G_Loss: 6.278223991394043\n",
      "Epoch 125\n",
      "Epoch: 125, Batch: 0, D_Loss: 0.10629173368215561, G_Loss: 4.610468864440918\n",
      "Epoch: 125, Batch: 100, D_Loss: 0.10749176144599915, G_Loss: 4.743229866027832\n",
      "Epoch: 125, Batch: 200, D_Loss: 0.1555134356021881, G_Loss: 4.642306327819824\n",
      "Epoch: 125, Batch: 300, D_Loss: 0.17342445254325867, G_Loss: 4.340571880340576\n",
      "Epoch: 125, Batch: 400, D_Loss: 0.09741836041212082, G_Loss: 4.797956466674805\n",
      "Epoch: 125, Batch: 500, D_Loss: 0.07820513099431992, G_Loss: 5.432340621948242\n",
      "Epoch 126\n",
      "Epoch: 126, Batch: 0, D_Loss: 0.09116294980049133, G_Loss: 4.355281829833984\n",
      "Epoch: 126, Batch: 100, D_Loss: 0.0498964749276638, G_Loss: 5.570218563079834\n",
      "Epoch: 126, Batch: 200, D_Loss: 0.17847664654254913, G_Loss: 4.796497344970703\n",
      "Epoch: 126, Batch: 300, D_Loss: 0.167861670255661, G_Loss: 3.8582653999328613\n",
      "Epoch: 126, Batch: 400, D_Loss: 0.0918007418513298, G_Loss: 5.010487079620361\n",
      "Epoch: 126, Batch: 500, D_Loss: 0.04418647661805153, G_Loss: 5.53929328918457\n",
      "Epoch 127\n",
      "Epoch: 127, Batch: 0, D_Loss: 0.0701332837343216, G_Loss: 4.842311382293701\n",
      "Epoch: 127, Batch: 100, D_Loss: 0.07522986084222794, G_Loss: 5.1065449714660645\n",
      "Epoch: 127, Batch: 200, D_Loss: 0.1321081817150116, G_Loss: 4.664127826690674\n",
      "Epoch: 127, Batch: 300, D_Loss: 0.09977155923843384, G_Loss: 4.116863250732422\n",
      "Epoch: 127, Batch: 400, D_Loss: 0.0972592830657959, G_Loss: 4.319211959838867\n",
      "Epoch: 127, Batch: 500, D_Loss: 0.08227606862783432, G_Loss: 5.410125732421875\n",
      "Epoch 128\n",
      "Epoch: 128, Batch: 0, D_Loss: 0.08756983280181885, G_Loss: 4.954030990600586\n",
      "Epoch: 128, Batch: 100, D_Loss: 0.08289839327335358, G_Loss: 4.925050735473633\n",
      "Epoch: 128, Batch: 200, D_Loss: 0.13319331407546997, G_Loss: 4.714478969573975\n",
      "Epoch: 128, Batch: 300, D_Loss: 0.13912886381149292, G_Loss: 4.328283786773682\n",
      "Epoch: 128, Batch: 400, D_Loss: 0.11943265795707703, G_Loss: 4.771146297454834\n",
      "Epoch: 128, Batch: 500, D_Loss: 0.051294177770614624, G_Loss: 5.284354209899902\n",
      "Epoch 129\n",
      "Epoch: 129, Batch: 0, D_Loss: 0.08327524363994598, G_Loss: 4.851931095123291\n",
      "Epoch: 129, Batch: 100, D_Loss: 0.08311550319194794, G_Loss: 5.181307315826416\n",
      "Epoch: 129, Batch: 200, D_Loss: 0.1259048879146576, G_Loss: 4.599881172180176\n",
      "Epoch: 129, Batch: 300, D_Loss: 0.10957985371351242, G_Loss: 4.099298477172852\n",
      "Epoch: 129, Batch: 400, D_Loss: 0.1305013746023178, G_Loss: 4.639552116394043\n",
      "Epoch: 129, Batch: 500, D_Loss: 0.078937828540802, G_Loss: 5.359806060791016\n",
      "Epoch 130\n",
      "Epoch: 130, Batch: 0, D_Loss: 0.07619426399469376, G_Loss: 4.710601329803467\n",
      "Epoch: 130, Batch: 100, D_Loss: 0.14635854959487915, G_Loss: 5.017481327056885\n",
      "Epoch: 130, Batch: 200, D_Loss: 0.1865120828151703, G_Loss: 4.7084126472473145\n",
      "Epoch: 130, Batch: 300, D_Loss: 0.11334696412086487, G_Loss: 4.071384429931641\n",
      "Epoch: 130, Batch: 400, D_Loss: 0.07145117223262787, G_Loss: 4.759936332702637\n",
      "Epoch: 130, Batch: 500, D_Loss: 0.04477238282561302, G_Loss: 6.101425647735596\n",
      "Epoch 131\n",
      "Epoch: 131, Batch: 0, D_Loss: 0.0866340696811676, G_Loss: 4.4140448570251465\n",
      "Epoch: 131, Batch: 100, D_Loss: 0.10254250466823578, G_Loss: 4.952829837799072\n",
      "Epoch: 131, Batch: 200, D_Loss: 0.11567539721727371, G_Loss: 5.008277416229248\n",
      "Epoch: 131, Batch: 300, D_Loss: 0.1379062831401825, G_Loss: 4.653039932250977\n",
      "Epoch: 131, Batch: 400, D_Loss: 0.08728346228599548, G_Loss: 4.488553047180176\n",
      "Epoch: 131, Batch: 500, D_Loss: 0.054895900189876556, G_Loss: 5.591744422912598\n",
      "Epoch 132\n",
      "Epoch: 132, Batch: 0, D_Loss: 0.1011877954006195, G_Loss: 4.836796760559082\n",
      "Epoch: 132, Batch: 100, D_Loss: 0.07927937060594559, G_Loss: 5.196883678436279\n",
      "Epoch: 132, Batch: 200, D_Loss: 0.16021515429019928, G_Loss: 5.50609016418457\n",
      "Epoch: 132, Batch: 300, D_Loss: 0.13443739712238312, G_Loss: 4.410562038421631\n",
      "Epoch: 132, Batch: 400, D_Loss: 0.07903342694044113, G_Loss: 5.166894435882568\n",
      "Epoch: 132, Batch: 500, D_Loss: 0.037665143609046936, G_Loss: 5.863004207611084\n",
      "Epoch 133\n",
      "Epoch: 133, Batch: 0, D_Loss: 0.08715040981769562, G_Loss: 4.573239803314209\n",
      "Epoch: 133, Batch: 100, D_Loss: 0.09148981422185898, G_Loss: 5.397617340087891\n",
      "Epoch: 133, Batch: 200, D_Loss: 0.12669236958026886, G_Loss: 4.61638069152832\n",
      "Epoch: 133, Batch: 300, D_Loss: 0.09126600623130798, G_Loss: 4.5915446281433105\n",
      "Epoch: 133, Batch: 400, D_Loss: 0.07210458815097809, G_Loss: 4.642611026763916\n",
      "Epoch: 133, Batch: 500, D_Loss: 0.051583610475063324, G_Loss: 5.558763027191162\n",
      "Epoch 134\n",
      "Epoch: 134, Batch: 0, D_Loss: 0.07747432589530945, G_Loss: 5.102482795715332\n",
      "Epoch: 134, Batch: 100, D_Loss: 0.11457845568656921, G_Loss: 4.506187915802002\n",
      "Epoch: 134, Batch: 200, D_Loss: 0.11780275404453278, G_Loss: 4.907071113586426\n",
      "Epoch: 134, Batch: 300, D_Loss: 0.09497739374637604, G_Loss: 4.225096225738525\n",
      "Epoch: 134, Batch: 400, D_Loss: 0.07490025460720062, G_Loss: 4.805346488952637\n",
      "Epoch: 134, Batch: 500, D_Loss: 0.04187805950641632, G_Loss: 5.7020769119262695\n",
      "Epoch 135\n",
      "Epoch: 135, Batch: 0, D_Loss: 0.08190573006868362, G_Loss: 4.62148904800415\n",
      "Epoch: 135, Batch: 100, D_Loss: 0.06086333468556404, G_Loss: 5.228367805480957\n",
      "Epoch: 135, Batch: 200, D_Loss: 0.13337454199790955, G_Loss: 4.625130653381348\n",
      "Epoch: 135, Batch: 300, D_Loss: 0.11134405434131622, G_Loss: 4.9083991050720215\n",
      "Epoch: 135, Batch: 400, D_Loss: 0.10049141943454742, G_Loss: 4.993565559387207\n",
      "Epoch: 135, Batch: 500, D_Loss: 0.06826197355985641, G_Loss: 5.431422233581543\n",
      "Epoch 136\n",
      "Epoch: 136, Batch: 0, D_Loss: 0.1147850900888443, G_Loss: 4.854089260101318\n",
      "Epoch: 136, Batch: 100, D_Loss: 0.059104837477207184, G_Loss: 5.207458972930908\n",
      "Epoch: 136, Batch: 200, D_Loss: 0.14726266264915466, G_Loss: 5.275691509246826\n",
      "Epoch: 136, Batch: 300, D_Loss: 0.08912637829780579, G_Loss: 4.414332389831543\n",
      "Epoch: 136, Batch: 400, D_Loss: 0.1843838095664978, G_Loss: 4.390864849090576\n",
      "Epoch: 136, Batch: 500, D_Loss: 0.05276399478316307, G_Loss: 6.267275333404541\n",
      "Epoch 137\n",
      "Epoch: 137, Batch: 0, D_Loss: 0.056799888610839844, G_Loss: 5.028393745422363\n",
      "Epoch: 137, Batch: 100, D_Loss: 0.06677588820457458, G_Loss: 4.7140793800354\n",
      "Epoch: 137, Batch: 200, D_Loss: 0.1318347454071045, G_Loss: 5.135262489318848\n",
      "Epoch: 137, Batch: 300, D_Loss: 0.07521344721317291, G_Loss: 4.3342604637146\n",
      "Epoch: 137, Batch: 400, D_Loss: 0.06818293035030365, G_Loss: 5.114500522613525\n",
      "Epoch: 137, Batch: 500, D_Loss: 0.04031500592827797, G_Loss: 6.001195907592773\n",
      "Epoch 138\n",
      "Epoch: 138, Batch: 0, D_Loss: 0.12954184412956238, G_Loss: 3.8928775787353516\n",
      "Epoch: 138, Batch: 100, D_Loss: 0.057425059378147125, G_Loss: 5.611563205718994\n",
      "Epoch: 138, Batch: 200, D_Loss: 0.1167680025100708, G_Loss: 4.813809871673584\n",
      "Epoch: 138, Batch: 300, D_Loss: 0.12030274420976639, G_Loss: 4.521421432495117\n",
      "Epoch: 138, Batch: 400, D_Loss: 0.1253802627325058, G_Loss: 4.383616924285889\n",
      "Epoch: 138, Batch: 500, D_Loss: 0.04334937781095505, G_Loss: 5.7667059898376465\n",
      "Epoch 139\n",
      "Epoch: 139, Batch: 0, D_Loss: 0.07919753342866898, G_Loss: 4.829962253570557\n",
      "Epoch: 139, Batch: 100, D_Loss: 0.05186481773853302, G_Loss: 5.727773666381836\n",
      "Epoch: 139, Batch: 200, D_Loss: 0.12952366471290588, G_Loss: 4.5533952713012695\n",
      "Epoch: 139, Batch: 300, D_Loss: 0.08835376799106598, G_Loss: 4.5353240966796875\n",
      "Epoch: 139, Batch: 400, D_Loss: 0.11121159046888351, G_Loss: 4.615799427032471\n",
      "Epoch: 139, Batch: 500, D_Loss: 0.04964836686849594, G_Loss: 5.77210807800293\n",
      "Epoch 140\n",
      "Epoch: 140, Batch: 0, D_Loss: 0.10725393891334534, G_Loss: 4.674696445465088\n",
      "Epoch: 140, Batch: 100, D_Loss: 0.05616166442632675, G_Loss: 5.0702924728393555\n",
      "Epoch: 140, Batch: 200, D_Loss: 0.13150623440742493, G_Loss: 4.725948333740234\n",
      "Epoch: 140, Batch: 300, D_Loss: 0.07065557688474655, G_Loss: 4.810725212097168\n",
      "Epoch: 140, Batch: 400, D_Loss: 0.07142999768257141, G_Loss: 4.817124843597412\n",
      "Epoch: 140, Batch: 500, D_Loss: 0.04067421704530716, G_Loss: 6.006199836730957\n",
      "Epoch 141\n",
      "Epoch: 141, Batch: 0, D_Loss: 0.09841590374708176, G_Loss: 4.8545145988464355\n",
      "Epoch: 141, Batch: 100, D_Loss: 0.09225716441869736, G_Loss: 5.155834197998047\n",
      "Epoch: 141, Batch: 200, D_Loss: 0.10202358663082123, G_Loss: 4.644014358520508\n",
      "Epoch: 141, Batch: 300, D_Loss: 0.08690553158521652, G_Loss: 4.2701616287231445\n",
      "Epoch: 141, Batch: 400, D_Loss: 0.11484820395708084, G_Loss: 4.9998698234558105\n",
      "Epoch: 141, Batch: 500, D_Loss: 0.07540351152420044, G_Loss: 5.573164463043213\n",
      "Epoch 142\n",
      "Epoch: 142, Batch: 0, D_Loss: 0.08442298322916031, G_Loss: 5.361844062805176\n",
      "Epoch: 142, Batch: 100, D_Loss: 0.07185446470975876, G_Loss: 5.5141730308532715\n",
      "Epoch: 142, Batch: 200, D_Loss: 0.10365605354309082, G_Loss: 4.906429290771484\n",
      "Epoch: 142, Batch: 300, D_Loss: 0.06713344156742096, G_Loss: 4.563847064971924\n",
      "Epoch: 142, Batch: 400, D_Loss: 0.08593704551458359, G_Loss: 5.0904622077941895\n",
      "Epoch: 142, Batch: 500, D_Loss: 0.029711652547121048, G_Loss: 6.461597919464111\n",
      "Epoch 143\n",
      "Epoch: 143, Batch: 0, D_Loss: 0.10635794699192047, G_Loss: 4.806825637817383\n",
      "Epoch: 143, Batch: 100, D_Loss: 0.07802197337150574, G_Loss: 4.662825107574463\n",
      "Epoch: 143, Batch: 200, D_Loss: 0.09071230888366699, G_Loss: 4.468173980712891\n",
      "Epoch: 143, Batch: 300, D_Loss: 0.11818964034318924, G_Loss: 4.611258029937744\n",
      "Epoch: 143, Batch: 400, D_Loss: 0.058312319219112396, G_Loss: 5.478222846984863\n",
      "Epoch: 143, Batch: 500, D_Loss: 0.04327927529811859, G_Loss: 6.0333662033081055\n",
      "Epoch 144\n",
      "Epoch: 144, Batch: 0, D_Loss: 0.12828963994979858, G_Loss: 4.514354705810547\n",
      "Epoch: 144, Batch: 100, D_Loss: 0.05008012056350708, G_Loss: 5.531408309936523\n",
      "Epoch: 144, Batch: 200, D_Loss: 0.12200577557086945, G_Loss: 4.940289497375488\n",
      "Epoch: 144, Batch: 300, D_Loss: 0.08392497897148132, G_Loss: 4.350164413452148\n",
      "Epoch: 144, Batch: 400, D_Loss: 0.07641078531742096, G_Loss: 5.145288467407227\n",
      "Epoch: 144, Batch: 500, D_Loss: 0.04658617079257965, G_Loss: 5.357892990112305\n",
      "Epoch 145\n",
      "Epoch: 145, Batch: 0, D_Loss: 0.07889257371425629, G_Loss: 4.805446147918701\n",
      "Epoch: 145, Batch: 100, D_Loss: 0.03895465284585953, G_Loss: 5.463282585144043\n",
      "Epoch: 145, Batch: 200, D_Loss: 0.14157913625240326, G_Loss: 4.362017631530762\n",
      "Epoch: 145, Batch: 300, D_Loss: 0.13174259662628174, G_Loss: 4.7786054611206055\n",
      "Epoch: 145, Batch: 400, D_Loss: 0.09875984489917755, G_Loss: 4.406607627868652\n",
      "Epoch: 145, Batch: 500, D_Loss: 0.04885062575340271, G_Loss: 5.454311370849609\n",
      "Epoch 146\n",
      "Epoch: 146, Batch: 0, D_Loss: 0.09103076159954071, G_Loss: 4.862710952758789\n",
      "Epoch: 146, Batch: 100, D_Loss: 0.07458722591400146, G_Loss: 5.448948383331299\n",
      "Epoch: 146, Batch: 200, D_Loss: 0.12332534790039062, G_Loss: 4.560292720794678\n",
      "Epoch: 146, Batch: 300, D_Loss: 0.07484745979309082, G_Loss: 4.929108619689941\n",
      "Epoch: 146, Batch: 400, D_Loss: 0.07302536070346832, G_Loss: 5.112884521484375\n",
      "Epoch: 146, Batch: 500, D_Loss: 0.06962332129478455, G_Loss: 5.600481986999512\n",
      "Epoch 147\n",
      "Epoch: 147, Batch: 0, D_Loss: 0.12261076271533966, G_Loss: 4.860601425170898\n",
      "Epoch: 147, Batch: 100, D_Loss: 0.06337792426347733, G_Loss: 4.9523115158081055\n",
      "Epoch: 147, Batch: 200, D_Loss: 0.09811810404062271, G_Loss: 5.05242919921875\n",
      "Epoch: 147, Batch: 300, D_Loss: 0.05600232258439064, G_Loss: 5.147409439086914\n",
      "Epoch: 147, Batch: 400, D_Loss: 0.07631491124629974, G_Loss: 4.974567413330078\n",
      "Epoch: 147, Batch: 500, D_Loss: 0.06301426887512207, G_Loss: 5.567899227142334\n",
      "Epoch 148\n",
      "Epoch: 148, Batch: 0, D_Loss: 0.09332618117332458, G_Loss: 4.867667198181152\n",
      "Epoch: 148, Batch: 100, D_Loss: 0.054752081632614136, G_Loss: 5.22178840637207\n",
      "Epoch: 148, Batch: 200, D_Loss: 0.1419684737920761, G_Loss: 4.5907769203186035\n",
      "Epoch: 148, Batch: 300, D_Loss: 0.08722308278083801, G_Loss: 4.781406879425049\n",
      "Epoch: 148, Batch: 400, D_Loss: 0.07912042737007141, G_Loss: 5.078701019287109\n",
      "Epoch: 148, Batch: 500, D_Loss: 0.0357368178665638, G_Loss: 5.9330153465271\n",
      "Epoch 149\n",
      "Epoch: 149, Batch: 0, D_Loss: 0.095664381980896, G_Loss: 5.253859996795654\n",
      "Epoch: 149, Batch: 100, D_Loss: 0.050206508487463, G_Loss: 5.330200672149658\n",
      "Epoch: 149, Batch: 200, D_Loss: 0.13713975250720978, G_Loss: 5.230486869812012\n",
      "Epoch: 149, Batch: 300, D_Loss: 0.07506848871707916, G_Loss: 5.264809608459473\n",
      "Epoch: 149, Batch: 400, D_Loss: 0.07414987683296204, G_Loss: 5.094186782836914\n",
      "Epoch: 149, Batch: 500, D_Loss: 0.04149320721626282, G_Loss: 5.6173095703125\n",
      "Epoch 150\n",
      "Epoch: 150, Batch: 0, D_Loss: 0.06592130661010742, G_Loss: 4.925273895263672\n",
      "Epoch: 150, Batch: 100, D_Loss: 0.04125596582889557, G_Loss: 5.4006500244140625\n",
      "Epoch: 150, Batch: 200, D_Loss: 0.09936115145683289, G_Loss: 4.728724956512451\n",
      "Epoch: 150, Batch: 300, D_Loss: 0.07094356417655945, G_Loss: 4.98460578918457\n",
      "Epoch: 150, Batch: 400, D_Loss: 0.0600719191133976, G_Loss: 4.759319305419922\n",
      "Epoch: 150, Batch: 500, D_Loss: 0.06497311592102051, G_Loss: 5.8295745849609375\n",
      "Epoch 151\n",
      "Epoch: 151, Batch: 0, D_Loss: 0.09579963982105255, G_Loss: 4.901251792907715\n",
      "Epoch: 151, Batch: 100, D_Loss: 0.07887092232704163, G_Loss: 4.894453525543213\n",
      "Epoch: 151, Batch: 200, D_Loss: 0.10791045427322388, G_Loss: 4.674911022186279\n",
      "Epoch: 151, Batch: 300, D_Loss: 0.06108911707997322, G_Loss: 5.299717903137207\n",
      "Epoch: 151, Batch: 400, D_Loss: 0.07277156412601471, G_Loss: 5.236199378967285\n",
      "Epoch: 151, Batch: 500, D_Loss: 0.059812992811203, G_Loss: 6.610147476196289\n",
      "Epoch 152\n",
      "Epoch: 152, Batch: 0, D_Loss: 0.11030006408691406, G_Loss: 4.786069869995117\n",
      "Epoch: 152, Batch: 100, D_Loss: 0.03865019604563713, G_Loss: 4.761096000671387\n",
      "Epoch: 152, Batch: 200, D_Loss: 0.09582352638244629, G_Loss: 4.6558074951171875\n",
      "Epoch: 152, Batch: 300, D_Loss: 0.0901499018073082, G_Loss: 5.125721454620361\n",
      "Epoch: 152, Batch: 400, D_Loss: 0.06022169440984726, G_Loss: 5.161687850952148\n",
      "Epoch: 152, Batch: 500, D_Loss: 0.041623130440711975, G_Loss: 6.219759464263916\n",
      "Epoch 153\n",
      "Epoch: 153, Batch: 0, D_Loss: 0.0815357118844986, G_Loss: 4.834103584289551\n",
      "Epoch: 153, Batch: 100, D_Loss: 0.05558726564049721, G_Loss: 4.998996257781982\n",
      "Epoch: 153, Batch: 200, D_Loss: 0.10147063434123993, G_Loss: 4.952154159545898\n",
      "Epoch: 153, Batch: 300, D_Loss: 0.05162675306200981, G_Loss: 4.759681224822998\n",
      "Epoch: 153, Batch: 400, D_Loss: 0.05311677232384682, G_Loss: 5.110108375549316\n",
      "Epoch: 153, Batch: 500, D_Loss: 0.038744933903217316, G_Loss: 6.205541610717773\n",
      "Epoch 154\n",
      "Epoch: 154, Batch: 0, D_Loss: 0.07990288734436035, G_Loss: 5.065591812133789\n",
      "Epoch: 154, Batch: 100, D_Loss: 0.056112319231033325, G_Loss: 4.576869487762451\n",
      "Epoch: 154, Batch: 200, D_Loss: 0.09250662475824356, G_Loss: 5.050504207611084\n",
      "Epoch: 154, Batch: 300, D_Loss: 0.04326828196644783, G_Loss: 5.022916316986084\n",
      "Epoch: 154, Batch: 400, D_Loss: 0.06221673637628555, G_Loss: 4.949859142303467\n",
      "Epoch: 154, Batch: 500, D_Loss: 0.04382293298840523, G_Loss: 5.974048614501953\n",
      "Epoch 155\n",
      "Epoch: 155, Batch: 0, D_Loss: 0.102849580347538, G_Loss: 4.639380931854248\n",
      "Epoch: 155, Batch: 100, D_Loss: 0.0581979900598526, G_Loss: 4.815062999725342\n",
      "Epoch: 155, Batch: 200, D_Loss: 0.08555874228477478, G_Loss: 4.937170505523682\n",
      "Epoch: 155, Batch: 300, D_Loss: 0.06863135099411011, G_Loss: 5.200659275054932\n",
      "Epoch: 155, Batch: 400, D_Loss: 0.07060419768095016, G_Loss: 4.940934658050537\n",
      "Epoch: 155, Batch: 500, D_Loss: 0.04611455276608467, G_Loss: 6.2264580726623535\n",
      "Epoch 156\n",
      "Epoch: 156, Batch: 0, D_Loss: 0.07720097154378891, G_Loss: 4.642106533050537\n",
      "Epoch: 156, Batch: 100, D_Loss: 0.06021697074174881, G_Loss: 5.088258266448975\n",
      "Epoch: 156, Batch: 200, D_Loss: 0.07148701697587967, G_Loss: 4.539816379547119\n",
      "Epoch: 156, Batch: 300, D_Loss: 0.051520124077796936, G_Loss: 5.679609775543213\n",
      "Epoch: 156, Batch: 400, D_Loss: 0.04409049451351166, G_Loss: 5.098435878753662\n",
      "Epoch: 156, Batch: 500, D_Loss: 0.04420694708824158, G_Loss: 6.773873805999756\n",
      "Epoch 157\n",
      "Epoch: 157, Batch: 0, D_Loss: 0.08788417279720306, G_Loss: 4.874844551086426\n",
      "Epoch: 157, Batch: 100, D_Loss: 0.06015341728925705, G_Loss: 4.9571967124938965\n",
      "Epoch: 157, Batch: 200, D_Loss: 0.09742778539657593, G_Loss: 4.796153545379639\n",
      "Epoch: 157, Batch: 300, D_Loss: 0.046667683869600296, G_Loss: 5.414254188537598\n",
      "Epoch: 157, Batch: 400, D_Loss: 0.07223194092512131, G_Loss: 4.837864875793457\n",
      "Epoch: 157, Batch: 500, D_Loss: 0.04130421578884125, G_Loss: 5.881843090057373\n",
      "Epoch 158\n",
      "Epoch: 158, Batch: 0, D_Loss: 0.10453206300735474, G_Loss: 4.6279449462890625\n",
      "Epoch: 158, Batch: 100, D_Loss: 0.04463996738195419, G_Loss: 5.126976490020752\n",
      "Epoch: 158, Batch: 200, D_Loss: 0.09654795378446579, G_Loss: 4.8796610832214355\n",
      "Epoch: 158, Batch: 300, D_Loss: 0.06233307719230652, G_Loss: 4.677754878997803\n",
      "Epoch: 158, Batch: 400, D_Loss: 0.05598856508731842, G_Loss: 5.057356357574463\n",
      "Epoch: 158, Batch: 500, D_Loss: 0.04175258427858353, G_Loss: 5.76713752746582\n",
      "Epoch 159\n",
      "Epoch: 159, Batch: 0, D_Loss: 0.12021337449550629, G_Loss: 4.8583831787109375\n",
      "Epoch: 159, Batch: 100, D_Loss: 0.03945848345756531, G_Loss: 5.252580642700195\n",
      "Epoch: 159, Batch: 200, D_Loss: 0.09445329010486603, G_Loss: 5.212641716003418\n",
      "Epoch: 159, Batch: 300, D_Loss: 0.06516017019748688, G_Loss: 5.358585357666016\n",
      "Epoch: 159, Batch: 400, D_Loss: 0.05948614701628685, G_Loss: 4.76176643371582\n",
      "Epoch: 159, Batch: 500, D_Loss: 0.026940561830997467, G_Loss: 5.849206447601318\n",
      "Epoch 160\n",
      "Epoch: 160, Batch: 0, D_Loss: 0.1254141628742218, G_Loss: 4.921087265014648\n",
      "Epoch: 160, Batch: 100, D_Loss: 0.03699851036071777, G_Loss: 5.60091495513916\n",
      "Epoch: 160, Batch: 200, D_Loss: 0.12016309797763824, G_Loss: 5.025373458862305\n",
      "Epoch: 160, Batch: 300, D_Loss: 0.05170421674847603, G_Loss: 4.760161399841309\n",
      "Epoch: 160, Batch: 400, D_Loss: 0.03875625878572464, G_Loss: 5.417918682098389\n",
      "Epoch: 160, Batch: 500, D_Loss: 0.04474863409996033, G_Loss: 6.152021408081055\n",
      "Epoch 161\n",
      "Epoch: 161, Batch: 0, D_Loss: 0.050194501876831055, G_Loss: 5.148186206817627\n",
      "Epoch: 161, Batch: 100, D_Loss: 0.056749895215034485, G_Loss: 4.881720066070557\n",
      "Epoch: 161, Batch: 200, D_Loss: 0.08968222141265869, G_Loss: 5.23483419418335\n",
      "Epoch: 161, Batch: 300, D_Loss: 0.03087574616074562, G_Loss: 5.525552749633789\n",
      "Epoch: 161, Batch: 400, D_Loss: 0.060168296098709106, G_Loss: 5.496142387390137\n",
      "Epoch: 161, Batch: 500, D_Loss: 0.042397476732730865, G_Loss: 6.564587593078613\n",
      "Epoch 162\n",
      "Epoch: 162, Batch: 0, D_Loss: 0.051770009100437164, G_Loss: 5.212411880493164\n",
      "Epoch: 162, Batch: 100, D_Loss: 0.04298907518386841, G_Loss: 4.933648109436035\n",
      "Epoch: 162, Batch: 200, D_Loss: 0.10340927541255951, G_Loss: 4.790961742401123\n",
      "Epoch: 162, Batch: 300, D_Loss: 0.044450271874666214, G_Loss: 5.384172439575195\n",
      "Epoch: 162, Batch: 400, D_Loss: 0.04564821347594261, G_Loss: 5.280816555023193\n",
      "Epoch: 162, Batch: 500, D_Loss: 0.03955668583512306, G_Loss: 6.132326602935791\n",
      "Epoch 163\n",
      "Epoch: 163, Batch: 0, D_Loss: 0.06465725600719452, G_Loss: 4.670731067657471\n",
      "Epoch: 163, Batch: 100, D_Loss: 0.06326047331094742, G_Loss: 5.186347484588623\n",
      "Epoch: 163, Batch: 200, D_Loss: 0.08430880308151245, G_Loss: 5.405057430267334\n",
      "Epoch: 163, Batch: 300, D_Loss: 0.040040649473667145, G_Loss: 5.28024435043335\n",
      "Epoch: 163, Batch: 400, D_Loss: 0.07687399536371231, G_Loss: 5.077669143676758\n",
      "Epoch: 163, Batch: 500, D_Loss: 0.052923254668712616, G_Loss: 6.087718963623047\n",
      "Epoch 164\n",
      "Epoch: 164, Batch: 0, D_Loss: 0.08771851658821106, G_Loss: 4.861485004425049\n",
      "Epoch: 164, Batch: 100, D_Loss: 0.03680145740509033, G_Loss: 5.742735385894775\n",
      "Epoch: 164, Batch: 200, D_Loss: 0.07208284735679626, G_Loss: 5.658050060272217\n",
      "Epoch: 164, Batch: 300, D_Loss: 0.04921150207519531, G_Loss: 5.35383415222168\n",
      "Epoch: 164, Batch: 400, D_Loss: 0.04779379069805145, G_Loss: 5.18923807144165\n",
      "Epoch: 164, Batch: 500, D_Loss: 0.03672265261411667, G_Loss: 5.664072036743164\n",
      "Epoch 165\n",
      "Epoch: 165, Batch: 0, D_Loss: 0.05171140283346176, G_Loss: 5.084986209869385\n",
      "Epoch: 165, Batch: 100, D_Loss: 0.04527411237359047, G_Loss: 5.285938739776611\n",
      "Epoch: 165, Batch: 200, D_Loss: 0.1084166020154953, G_Loss: 4.701108932495117\n",
      "Epoch: 165, Batch: 300, D_Loss: 0.034035664051771164, G_Loss: 5.387614727020264\n",
      "Epoch: 165, Batch: 400, D_Loss: 0.047573454678058624, G_Loss: 5.107187747955322\n",
      "Epoch: 165, Batch: 500, D_Loss: 0.04054835066199303, G_Loss: 5.789346694946289\n",
      "Epoch 166\n",
      "Epoch: 166, Batch: 0, D_Loss: 0.10386710613965988, G_Loss: 4.8964762687683105\n",
      "Epoch: 166, Batch: 100, D_Loss: 0.03663216531276703, G_Loss: 5.153634071350098\n",
      "Epoch: 166, Batch: 200, D_Loss: 0.08855335414409637, G_Loss: 5.302756309509277\n",
      "Epoch: 166, Batch: 300, D_Loss: 0.047252804040908813, G_Loss: 5.3530144691467285\n",
      "Epoch: 166, Batch: 400, D_Loss: 0.05565051734447479, G_Loss: 5.0129852294921875\n",
      "Epoch: 166, Batch: 500, D_Loss: 0.04106671363115311, G_Loss: 6.728604316711426\n",
      "Epoch 167\n",
      "Epoch: 167, Batch: 0, D_Loss: 0.07310093939304352, G_Loss: 4.607651233673096\n",
      "Epoch: 167, Batch: 100, D_Loss: 0.04595411568880081, G_Loss: 5.156493663787842\n",
      "Epoch: 167, Batch: 200, D_Loss: 0.09551222622394562, G_Loss: 5.261043071746826\n",
      "Epoch: 167, Batch: 300, D_Loss: 0.04058157280087471, G_Loss: 5.580963611602783\n",
      "Epoch: 167, Batch: 400, D_Loss: 0.07163326442241669, G_Loss: 5.010891437530518\n",
      "Epoch: 167, Batch: 500, D_Loss: 0.04434605687856674, G_Loss: 6.449911117553711\n",
      "Epoch 168\n",
      "Epoch: 168, Batch: 0, D_Loss: 0.0701032429933548, G_Loss: 4.87825870513916\n",
      "Epoch: 168, Batch: 100, D_Loss: 0.05490287020802498, G_Loss: 4.661071300506592\n",
      "Epoch: 168, Batch: 200, D_Loss: 0.08076134324073792, G_Loss: 4.7885661125183105\n",
      "Epoch: 168, Batch: 300, D_Loss: 0.04358724504709244, G_Loss: 5.34913444519043\n",
      "Epoch: 168, Batch: 400, D_Loss: 0.047875627875328064, G_Loss: 5.723881244659424\n",
      "Epoch: 168, Batch: 500, D_Loss: 0.04271269589662552, G_Loss: 6.329935073852539\n",
      "Epoch 169\n",
      "Epoch: 169, Batch: 0, D_Loss: 0.052263855934143066, G_Loss: 5.2465338706970215\n",
      "Epoch: 169, Batch: 100, D_Loss: 0.029744068160653114, G_Loss: 5.507928371429443\n",
      "Epoch: 169, Batch: 200, D_Loss: 0.07103808224201202, G_Loss: 5.501581192016602\n",
      "Epoch: 169, Batch: 300, D_Loss: 0.024077843874692917, G_Loss: 5.770855903625488\n",
      "Epoch: 169, Batch: 400, D_Loss: 0.04208366572856903, G_Loss: 5.718357563018799\n",
      "Epoch: 169, Batch: 500, D_Loss: 0.0345245897769928, G_Loss: 5.595941066741943\n",
      "Epoch 170\n",
      "Epoch: 170, Batch: 0, D_Loss: 0.06410929560661316, G_Loss: 4.3362226486206055\n",
      "Epoch: 170, Batch: 100, D_Loss: 0.03454539552330971, G_Loss: 5.257335186004639\n",
      "Epoch: 170, Batch: 200, D_Loss: 0.07985936850309372, G_Loss: 4.915124893188477\n",
      "Epoch: 170, Batch: 300, D_Loss: 0.026808228343725204, G_Loss: 5.519538402557373\n",
      "Epoch: 170, Batch: 400, D_Loss: 0.054310016334056854, G_Loss: 4.792651653289795\n",
      "Epoch: 170, Batch: 500, D_Loss: 0.02845037914812565, G_Loss: 6.638279914855957\n",
      "Epoch 171\n",
      "Epoch: 171, Batch: 0, D_Loss: 0.06393365561962128, G_Loss: 4.869217395782471\n",
      "Epoch: 171, Batch: 100, D_Loss: 0.053327783942222595, G_Loss: 4.816325664520264\n",
      "Epoch: 171, Batch: 200, D_Loss: 0.08838585764169693, G_Loss: 5.484532356262207\n",
      "Epoch: 171, Batch: 300, D_Loss: 0.020636580884456635, G_Loss: 5.540753364562988\n",
      "Epoch: 171, Batch: 400, D_Loss: 0.05604204162955284, G_Loss: 5.128271102905273\n",
      "Epoch: 171, Batch: 500, D_Loss: 0.02914666198194027, G_Loss: 6.701600551605225\n",
      "Epoch 172\n",
      "Epoch: 172, Batch: 0, D_Loss: 0.07018616795539856, G_Loss: 4.83933162689209\n",
      "Epoch: 172, Batch: 100, D_Loss: 0.0336872823536396, G_Loss: 4.911392688751221\n",
      "Epoch: 172, Batch: 200, D_Loss: 0.06747516989707947, G_Loss: 5.5249223709106445\n",
      "Epoch: 172, Batch: 300, D_Loss: 0.025627270340919495, G_Loss: 5.294684886932373\n",
      "Epoch: 172, Batch: 400, D_Loss: 0.04054030776023865, G_Loss: 4.757330417633057\n",
      "Epoch: 172, Batch: 500, D_Loss: 0.038997918367385864, G_Loss: 6.024467945098877\n",
      "Epoch 173\n",
      "Epoch: 173, Batch: 0, D_Loss: 0.05600958317518234, G_Loss: 5.087221145629883\n",
      "Epoch: 173, Batch: 100, D_Loss: 0.02404390648007393, G_Loss: 5.7213568687438965\n",
      "Epoch: 173, Batch: 200, D_Loss: 0.07496108114719391, G_Loss: 5.011139392852783\n",
      "Epoch: 173, Batch: 300, D_Loss: 0.03491660952568054, G_Loss: 5.718574047088623\n",
      "Epoch: 173, Batch: 400, D_Loss: 0.049713388085365295, G_Loss: 5.460778713226318\n",
      "Epoch: 173, Batch: 500, D_Loss: 0.027867401018738747, G_Loss: 6.300632476806641\n",
      "Epoch 174\n",
      "Epoch: 174, Batch: 0, D_Loss: 0.06988215446472168, G_Loss: 4.81089973449707\n",
      "Epoch: 174, Batch: 100, D_Loss: 0.04063459485769272, G_Loss: 5.632458686828613\n",
      "Epoch: 174, Batch: 200, D_Loss: 0.059722453355789185, G_Loss: 5.526629447937012\n",
      "Epoch: 174, Batch: 300, D_Loss: 0.03725271672010422, G_Loss: 5.771047592163086\n",
      "Epoch: 174, Batch: 400, D_Loss: 0.03601401671767235, G_Loss: 5.775576114654541\n",
      "Epoch: 174, Batch: 500, D_Loss: 0.02425994724035263, G_Loss: 6.287442684173584\n",
      "Epoch 175\n",
      "Epoch: 175, Batch: 0, D_Loss: 0.04002697393298149, G_Loss: 5.58427619934082\n",
      "Epoch: 175, Batch: 100, D_Loss: 0.04156529903411865, G_Loss: 5.142056941986084\n",
      "Epoch: 175, Batch: 200, D_Loss: 0.09443613886833191, G_Loss: 6.029680252075195\n",
      "Epoch: 175, Batch: 300, D_Loss: 0.02823473885655403, G_Loss: 5.716012001037598\n",
      "Epoch: 175, Batch: 400, D_Loss: 0.03429904207587242, G_Loss: 5.53681755065918\n",
      "Epoch: 175, Batch: 500, D_Loss: 0.02722557634115219, G_Loss: 6.212775230407715\n",
      "Epoch 176\n",
      "Epoch: 176, Batch: 0, D_Loss: 0.08090473711490631, G_Loss: 4.448620796203613\n",
      "Epoch: 176, Batch: 100, D_Loss: 0.02751735784113407, G_Loss: 4.9995832443237305\n",
      "Epoch: 176, Batch: 200, D_Loss: 0.04796513542532921, G_Loss: 5.714567184448242\n",
      "Epoch: 176, Batch: 300, D_Loss: 0.03441000357270241, G_Loss: 5.795243740081787\n",
      "Epoch: 176, Batch: 400, D_Loss: 0.07295916974544525, G_Loss: 5.035201549530029\n",
      "Epoch: 176, Batch: 500, D_Loss: 0.024812791496515274, G_Loss: 6.696315288543701\n",
      "Epoch 177\n",
      "Epoch: 177, Batch: 0, D_Loss: 0.07567258179187775, G_Loss: 4.823994159698486\n",
      "Epoch: 177, Batch: 100, D_Loss: 0.031134992837905884, G_Loss: 5.011638164520264\n",
      "Epoch: 177, Batch: 200, D_Loss: 0.05112319812178612, G_Loss: 5.197466850280762\n",
      "Epoch: 177, Batch: 300, D_Loss: 0.035782553255558014, G_Loss: 5.707179546356201\n",
      "Epoch: 177, Batch: 400, D_Loss: 0.042120225727558136, G_Loss: 5.2733049392700195\n",
      "Epoch: 177, Batch: 500, D_Loss: 0.03413209691643715, G_Loss: 8.777990341186523\n",
      "Epoch 178\n",
      "Epoch: 178, Batch: 0, D_Loss: 0.06912726163864136, G_Loss: 4.946635723114014\n",
      "Epoch: 178, Batch: 100, D_Loss: 0.02797437645494938, G_Loss: 4.980381965637207\n",
      "Epoch: 178, Batch: 200, D_Loss: 0.055163152515888214, G_Loss: 5.895996570587158\n",
      "Epoch: 178, Batch: 300, D_Loss: 0.02530086785554886, G_Loss: 6.592067718505859\n",
      "Epoch: 178, Batch: 400, D_Loss: 0.04004330560564995, G_Loss: 5.135122299194336\n",
      "Epoch: 178, Batch: 500, D_Loss: 0.013920285739004612, G_Loss: 6.423823356628418\n",
      "Epoch 179\n",
      "Epoch: 179, Batch: 0, D_Loss: 0.04871440678834915, G_Loss: 4.769067287445068\n",
      "Epoch: 179, Batch: 100, D_Loss: 0.025197716429829597, G_Loss: 5.518400192260742\n",
      "Epoch: 179, Batch: 200, D_Loss: 0.053246110677719116, G_Loss: 5.496625900268555\n",
      "Epoch: 179, Batch: 300, D_Loss: 0.06323255598545074, G_Loss: 5.320883750915527\n",
      "Epoch: 179, Batch: 400, D_Loss: 0.04427012428641319, G_Loss: 5.852653980255127\n",
      "Epoch: 179, Batch: 500, D_Loss: 0.02326025627553463, G_Loss: 6.872926235198975\n",
      "Epoch 180\n",
      "Epoch: 180, Batch: 0, D_Loss: 0.06525435298681259, G_Loss: 5.143326282501221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6037e3e9f0dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mimages_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_noise\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         _, d_error, d_pred_real, d_pred_fake = session.run(\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0md_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_fake\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         )\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def next_batch(arr, batch_size):\n",
    "    num_batches = int(len(arr) / batch_size)\n",
    "    for i in range(0, num_batches * batch_size, batch_size):\n",
    "        yield arr[i:i+batch_size]\n",
    "    yield arr[num_batches*batch_size:]\n",
    "\n",
    "# Iterate through epochs\n",
    "BATCH_SIZE = 100\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    for n_batch, batch in enumerate(next_batch(train_X, BATCH_SIZE)):\n",
    "        # 1. Train Discriminator\n",
    "        #X_batch = images_to_vectors(batch.permute(0, 2, 3, 1).numpy())\n",
    "        batch_noise = np.random.uniform(-1, 1, [BATCH_SIZE, NOISE_SIZE]).astype(np.float32)\n",
    "        feed_dict = {images_input: batch, noise_input: batch_noise}\n",
    "        _, d_error, d_pred_real, d_pred_fake = session.run(\n",
    "            [d_opt, d_loss, d_real, d_fake], feed_dict=feed_dict\n",
    "        )\n",
    "\n",
    "        # 2. Train Generator\n",
    "        feed_dict = {noise_input: batch_noise}\n",
    "        _, g_error = session.run(\n",
    "            [g_opt, g_loss], feed_dict=feed_dict\n",
    "        )\n",
    "\n",
    "        if n_batch % 100 == 0:\n",
    "        #    display.clear_output(True)\n",
    "            # Generate images from test noise\n",
    "            test_images, gen_image_summary, gen_image_summary1, gen_image_summary2, gen_image_summary3 = session.run(\n",
    "                [g_sample, gen_image_summary_op, gen_image_summary_op1, gen_image_summary_op2, gen_image_summary_op3], feed_dict={noise_input: test_noise}\n",
    "            )\n",
    "            writer.add_summary(gen_image_summary, global_step=epoch*1000+n_batch)\n",
    "            writer.add_summary(gen_image_summary1, global_step=epoch*1000+n_batch)\n",
    "            writer.add_summary(gen_image_summary2, global_step=epoch*1000+n_batch)\n",
    "            writer.add_summary(gen_image_summary3, global_step=epoch*1000+n_batch)\n",
    "            \n",
    "            print(\"Epoch: {}, Batch: {}, D_Loss: {}, G_Loss: {}\".format(epoch, n_batch, d_error, g_error))\n",
    "        #    test_images = vectors_to_images(test_images)\n",
    "        #    # Log Images\n",
    "        #    logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches, format='NHWC');\n",
    "        #    # Log Status\n",
    "        #    logger.display_status(\n",
    "        #        epoch, num_epochs, n_batch, num_batches,\n",
    "        #        d_error, g_error, d_pred_real, d_pred_fake\n",
    "        #    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_1:0' shape=(?, 784) dtype=float32>, <tf.Tensor 'IteratorGetNext_1:1' shape=(?, 10) dtype=float64>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
