{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fc0svhAvL2E"
   },
   "source": [
    "### Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1520212837341,
     "user": {
      "displayName": "Ondrej Skopek",
      "photoUrl": "//lh6.googleusercontent.com/-WGkwqACs-x0/AAAAAAAAAAI/AAAAAAAACk8/KLm0zyt0xu0/s50-c-k-no/photo.jpg",
      "userId": "110435012592377289802"
     },
     "user_tz": -60
    },
    "id": "OWJxXDk0sPY7",
    "outputId": "231714fe-e5e3-4d10-f2a9-f767c4dfc83b"
   },
   "outputs": [],
   "source": [
    "# noqa\n",
    "import os\n",
    "COLAB = 'DATALAB_DEBUG' in os.environ\n",
    "if COLAB:\n",
    "    %cd /content\n",
    "    ROOT_DIR = '/content'\n",
    "    REPO_DIR = os.path.join(ROOT_DIR, 'ml_project_template')\n",
    "    LOG_DIR = os.path.join(REPO_DIR, 'data_out')\n",
    "\n",
    "    if not os.path.isdir(REPO_DIR):\n",
    "      !git clone https://github.com/oskopek/ml_project_template.git\n",
    "    if not os.path.isdir(LOG_DIR):\n",
    "      os.makedirs(LOG_DIR)\n",
    "    %cd 'ml_project_template'\n",
    "    !git pull\n",
    "    %ls\n",
    "else:\n",
    "    wd = %pwd\n",
    "    print('Current directory:', wd)\n",
    "    if wd.endswith('notebooks'):\n",
    "        %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQbS6L9Gwo3m"
   },
   "source": [
    "### Install missing packages\n",
    "\n",
    "Do not want to do `pip install -r requirements.txt` because that will overwrite the versions on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_kLthw9hwoGU"
   },
   "outputs": [],
   "source": [
    "# noqa\n",
    "if COLAB:\n",
    "    !pip install dotmap==1.2.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll72CJURwdgX"
   },
   "source": [
    "### Branch selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1520212841227,
     "user": {
      "displayName": "Ondrej Skopek",
      "photoUrl": "//lh6.googleusercontent.com/-WGkwqACs-x0/AAAAAAAAAAI/AAAAAAAACk8/KLm0zyt0xu0/s50-c-k-no/photo.jpg",
      "userId": "110435012592377289802"
     },
     "user_tz": -60
    },
    "id": "QSaOfz51tgq0",
    "outputId": "52ad7b1d-55fe-452e-8b0c-1a0c10d064a1"
   },
   "outputs": [],
   "source": [
    "# noqa\n",
    "if COLAB:\n",
    "    !git checkout master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldJXLaOZwzu_"
   },
   "source": [
    "### Tensorboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "output_extras": [
      {
       "item_id": 3
      },
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3658,
     "status": "ok",
     "timestamp": 1520212899593,
     "user": {
      "displayName": "Ondrej Skopek",
      "photoUrl": "//lh6.googleusercontent.com/-WGkwqACs-x0/AAAAAAAAAAI/AAAAAAAACk8/KLm0zyt0xu0/s50-c-k-no/photo.jpg",
      "userId": "110435012592377289802"
     },
     "user_tz": -60
    },
    "id": "VreinDV9u-Dm",
    "outputId": "a4a3912f-8808-4903-d8cd-fb219c85d938"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    import os\n",
    "    import resources.colab_utils.tboard as tboard\n",
    "\n",
    "    # will install `ngrok`, if necessary\n",
    "    # will create `log_dir` if path does not exist\n",
    "    tboard.launch_tensorboard(bin_dir=REPO_DIR, log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST example notebook\n",
    "\n",
    "This is an example of how a notebook for an ML task should be structured when using TF Eager Execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noqa\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow.contrib.summary as tf_summary\n",
    "\n",
    "try:\n",
    "    tfe.enable_eager_execution()\n",
    "except ValueError:\n",
    "    print('Eager exec already enabled.')\n",
    "\n",
    "from models.base import BaseModel\n",
    "\n",
    "# Flags\n",
    "from flags import flags_parser\n",
    "flags_parser.parse('flags/cnn.json', None)\n",
    "FLAGS = flags_parser.FLAGS\n",
    "assert FLAGS is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the input data\n",
    "\n",
    "In this case, MNIST + batch and shuffle it. In our case, it will be quite different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Returns training and test tf.data.Dataset objects.\"\"\"\n",
    "    data = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels))\n",
    "    test_ds = tf.data.Dataset.from_tensors((data.test.images, data.test.labels))\n",
    "    return (train_ds, test_ds)\n",
    "\n",
    "\n",
    "device, data_format = ('/gpu:0', 'channels_first')\n",
    "if FLAGS.no_gpu or tfe.num_gpus() <= 0:\n",
    "    device, data_format = ('/cpu:0', 'channels_last')\n",
    "print('Using device %s, and data format %s.' % (device, data_format))\n",
    "\n",
    "# Load the datasets\n",
    "train_ds, test_ds = load_data(FLAGS.data.in_dir)\n",
    "train_ds = train_ds.shuffle(60000).batch(FLAGS.model.optimization.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tfe.Network):\n",
    "    \"\"\"MNIST model.\n",
    "    Network structure is equivalent to:\n",
    "    https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_deep.py\n",
    "    and\n",
    "    https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py\n",
    "\n",
    "    But written using the tf.layers API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_format, name=''):\n",
    "        \"\"\"Creates a model for classifying a hand-written digit.\n",
    "\n",
    "        Args:\n",
    "          data_format: Either 'channels_first' or 'channels_last'.\n",
    "            'channels_first' is typically faster on GPUs while 'channels_last' is\n",
    "            typically faster on CPUs. See\n",
    "            https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "        \"\"\"\n",
    "        super(MNISTModel, self).__init__()\n",
    "        conv_size = 32\n",
    "        if data_format == 'channels_first':\n",
    "            self._input_shape = [-1, 1, 28, 28]\n",
    "        else:\n",
    "            assert data_format == 'channels_last'\n",
    "            self._input_shape = [-1, 28, 28, 1]\n",
    "        self.conv1 = self.track_layer(tf.layers.Conv2D(conv_size, 5, data_format=data_format, activation=tf.nn.relu))\n",
    "        self.conv2 = self.track_layer(\n",
    "            tf.layers.Conv2D(conv_size * 2, 5, data_format=data_format, activation=tf.nn.relu))\n",
    "        self.fc1 = self.track_layer(tf.layers.Dense(1024, activation=tf.nn.relu))\n",
    "        self.fc2 = self.track_layer(tf.layers.Dense(10))\n",
    "        self.dropout = self.track_layer(tf.layers.Dropout(0.5))\n",
    "        self.max_pool2d = self.track_layer(\n",
    "            tf.layers.MaxPooling2D((2, 2), (2, 2), padding='SAME', data_format=data_format))\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        \"\"\"Computes labels from inputs.\n",
    "\n",
    "        Users should invoke __call__ to run the network, which delegates to this\n",
    "        method (and not call this method directly).\n",
    "\n",
    "        Args:\n",
    "          inputs: A batch of images as a Tensor with shape [batch_size, 784].\n",
    "          training: True if invoked in the context of training (causing dropout to\n",
    "            be applied).  False otherwise.\n",
    "\n",
    "        Returns:\n",
    "          A Tensor with shape [batch_size, 10] containing the predicted logits\n",
    "          for each image in the batch, for each of the 10 classes.\n",
    "        \"\"\"\n",
    "\n",
    "        x = tf.reshape(inputs, self._input_shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    print(\"logits:\", tf.shape(logits))\n",
    "    print(\"labels:\", tf.shape(labels))\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    batch_size = int(logits.shape[0])\n",
    "    return tf.reduce_sum(tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataset, log_interval=None):\n",
    "    \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    start = time.time()\n",
    "    for (batch, (images, labels)) in enumerate(tfe.Iterator(dataset)):\n",
    "        with tf_summary.record_summaries_every_n_global_steps(10):\n",
    "            # Record the operations used to compute the loss given the input,\n",
    "            # so that the gradient of the loss with respect to the variables\n",
    "            # can be computed.\n",
    "            with tfe.GradientTape() as tape:\n",
    "                logits = model(images, training=True)\n",
    "                loss_value = loss(logits=logits, labels=labels)\n",
    "                tf_summary.scalar('loss', loss_value)\n",
    "                tf_summary.scalar('accuracy', compute_accuracy(logits=logits, labels=labels))\n",
    "            grads = tape.gradient(loss_value, model.variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables), global_step=global_step)\n",
    "\n",
    "            if log_interval and batch % log_interval == 0:\n",
    "                rate = log_interval / (time.time() - start)\n",
    "                print('Step #%d\\tLoss: %.6f (%d steps/sec)' % (batch, loss_value, rate))\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset):\n",
    "    \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
    "    avg_loss = tfe.metrics.Mean('loss')\n",
    "    accuracy = tfe.metrics.Accuracy('accuracy')\n",
    "\n",
    "    for (images, labels) in tfe.Iterator(dataset):\n",
    "        logits = model(images, training=False)\n",
    "        avg_loss(loss(logits, labels))\n",
    "        accuracy(tf.argmax(logits, axis=1, output_type=tf.int64), tf.cast(labels, tf.int64))\n",
    "\n",
    "    print('Test set: Average loss: %.4f, Accuracy: %4f%%\\n' % (avg_loss.result(), 100 * accuracy.result()))\n",
    "    with tf_summary.always_record_summaries():\n",
    "        tf_summary.scalar('loss', avg_loss.result())\n",
    "        tf_summary.scalar('accuracy', accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique experiment name for each run:\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "expname = \"MNIST_conv32_64_fc1024_10-{}\".format(timestamp)\n",
    "\n",
    "# Create the model and optimizer\n",
    "model = MNISTModel(data_format)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.model.optimization.learning_rate)\n",
    "\n",
    "if FLAGS.data.out_dir:\n",
    "    train_dir = os.path.join(FLAGS.data.out_dir, expname)\n",
    "    test_dir = os.path.join(FLAGS.data.out_dir, expname, 'eval')\n",
    "    tf.gfile.MakeDirs(FLAGS.data.out_dir)\n",
    "else:\n",
    "    train_dir = None\n",
    "    test_dir = None\n",
    "\n",
    "summary_writer = tf_summary.create_file_writer(train_dir, flush_millis=10000, name='train')\n",
    "test_summary_writer = tf_summary.create_file_writer(test_dir, flush_millis=10000, name='test')\n",
    "checkpoint_dir = os.path.join(train_dir, 'ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate for 11 epochs.\n",
    "# TODO: Fix this, sometime in the future, when the API changes 10 more times.\n",
    "with tf.device(device):\n",
    "    for epoch in range(FLAGS.model.optimization.epochs):\n",
    "        with tfe.restore_variables_on_create(tf.train.latest_checkpoint(checkpoint_dir)):\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "            start = time.time()\n",
    "            with summary_writer.as_default():\n",
    "                train(model, optimizer, train_ds, log_interval=FLAGS.training.log_interval)\n",
    "            end = time.time()\n",
    "            print('\\nTrain time for epoch #%d (global step %d): %f' % (epoch, global_step.numpy(), end - start))\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            test(model, test_ds)\n",
    "\n",
    "        all_variables = (model.variables + optimizer.variables() + [global_step])\n",
    "        tfe.Saver(all_variables).save(checkpoint_prefix, global_step=global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
